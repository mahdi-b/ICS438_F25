{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Identifying similar items\n",
    "\n",
    "* A fundamental problem in data mining is to search for \"similar\" items.\n",
    "* E.g.: \n",
    "    * Finding duplicate Web pages \n",
    "    * Finding duplicate listings on an e-commerce platform (e.g., Craigslist or Amazon)\n",
    "   \n",
    "* How do we find very closely related items to a given query $q$\n",
    "* We often need to do that for all instances of a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Native Solution\n",
    "\n",
    "* Compare items pairwise and compute some score of similarity between the pages.\n",
    "\n",
    "* A naive approach is not tractable.\n",
    "\n",
    "* For n items there are $\\frac{n \\times (n-1)}{2} \\in O(n^2)$ unique comparisons\n",
    "  * With 1 million items, there are 4,999,9950,0000 comparisons\n",
    "    * That's a relatively small dataset by today's standard\n",
    "  * May be computationally intractable or cost-prohibitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Score of Between Two Items\n",
    "\n",
    "* Suppose we have two records (instances) for a dataset\n",
    "  * Records contain `marital_status`, `number of children`, `has_investments?`, `owns_house?`, `car >= 2?`\n",
    "\n",
    "```python    \n",
    " a = [\"Married\", \"4\", \"yes\", \"yes\", \"yes\"]\n",
    " b = [\"Married\", \"2\", \"yes\", \"yes\", \"yes\"]\n",
    " c = [\"Single\",  \"0\", \"yes\", \"no\", \"no\"]\n",
    " ``` \n",
    "* How similar are the following sets?\n",
    "* We can see that `a` and `b` are more alike than `a` and `c` or `a` and `d`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Similarity Between two Sets\n",
    "\n",
    "```python\n",
    " a = [\"Married\", \"4\", \"yes\", \"yes\", \"yes\"]\n",
    " b = [\"Married\", \"2\", \"yes\", \"yes\", \"yes\"]\n",
    "```\n",
    "\n",
    "* One way to capture the similarity is by comparing the number of shared features\n",
    "* `a` and `b share 4 similar values out of 5.\n",
    "* `a` and `c` share 1 similar value out of 5.\n",
    "\n",
    "* We can therefore say that `a` and `b` are more similar than `a and c` or `b` and `c`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Jaccard Similarity\n",
    "\n",
    "* We define the Jaccard similarity between two sets $S$ and $T is \n",
    "\n",
    "$$\n",
    "J(S,T) = \\frac{|S \\cap T |}{|S \\cup T |}\n",
    "$$\n",
    "\n",
    "  * i.e., the size of the intersection of S and T to the size of their union. \n",
    "\n",
    "* $J(a,b) = 4/5 = 0.8$\n",
    "* $J(a,c) = 1/5 = 0.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Hashing\n",
    "\n",
    "* Many applications depend critically on quickly finding items in a list\n",
    "  * We cannot afford to search for the item (Binary search is $O(logN)$ where N is the number of items to search) \n",
    "  * E.g.: air traffic control or packet routing in critical web applications\n",
    "* Hashing can yield a match in near-constant time O(1)\n",
    "  * The cost to compute the hash is constant\n",
    "\n",
    "* Idea: \n",
    "  1. Transform the searched-for item into some index (key)\n",
    "    * We can compute the `key` of an element `e` using a funciton `key()`\n",
    "    * `key(e)= k`\n",
    "  2. Find the index in a table (table)\n",
    "    * Use the hash function to determine bin (`b`) in a table where `k` belongs.\n",
    "    * `hash(b)= b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Hashing -- Cont'd \n",
    "<img src=\"https://www.dropbox.com/s/tkk8tz0yy6k7v2q/hashing.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Universally Unique Identifier (UUID) \n",
    "### are often used to identify objects in software\n",
    "### Paricularly in\n",
    "import uuid\n",
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_list = []\n",
    "for val in range(5):\n",
    "    uuid_list.append(str(uuid.uuid4()))\n",
    "uuid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following uses a list comprehension.\n",
    "# List comprehensions are useful and terse to write \n",
    "# See https://realpython.com/list-comprehension-python/\n",
    "uuid_list = [str(uuid.uuid4()) for i in range(5)] \n",
    "uuid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "uuid_list = [(str(uuid.uuid4()), time.time()) for i in range(5)] \n",
    "uuid_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "uuid_list = [(str(uuid.uuid4()), time.time()) for i in range(1_000_000)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_list[999999][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = uuid_list[999999][0]\n",
    "for elem in uuid_list:\n",
    "    if elem[0] == query:\n",
    "        print(elem)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "[x for x in uuid_list if x[0] == query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(0, 999_999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.randint(0, 999_999)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "for i in range(100):\n",
    "    rand_position = random.randint(0, 999_999)\n",
    "    query = uuid_list[rand_position][0]\n",
    "    queries.append(query)\n",
    "\n",
    "queries[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for q in queries:\n",
    "    temp = [x for x in uuid_list if x[0] == q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily build a dict from a list of tuples\n",
    "dict([(\"A\", 1), (\"B\", 2), (\"C\", 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily build a dict from a list of tuples\n",
    "some_dict = dict([(\"A\", 1), (\"B\", 2), (\"C\", 3)])\n",
    "some_dict[\"B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "uuid_hash = dict(uuid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = queries[0]\n",
    "print(f\"The query is: {q}\")\n",
    "print(f\"The time associated with q is {uuid_hash[q]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for q in queries:\n",
    "    uuid_hash[q]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### The Hash Function in Python\n",
    "\n",
    "* Python uses `hash()` to hash an immutable object\n",
    "  * Combine conversion of input to a key and hash of key to a bin\n",
    "  * Cannot hash any object that can be modified, such as lists\n",
    "  * The has value is used to determine the location (address) where a dict key will be stored\n",
    "  \n",
    "```python\n",
    "    hash()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Hashing Similar Items\n",
    "\n",
    "* The following dataset contains individuals' info\n",
    "  * `name`, `age`, `salary`, and `number of years of experience`\n",
    "\n",
    "```python\n",
    "data_1 = (\"John\", \"Doe\", \"32\", \"165,385\", 3)\n",
    "data_2 = (\"Jane\", \"Doe\", \"32\", \"192,891\", 3)\n",
    "data_3 = (\"Mark\", \"Smith\", \"34\", \"85,232\", 2)\n",
    "```\n",
    "\n",
    "* We can hash each of these datasets using the `hash` function.\n",
    "  * Note that I declared them as tuples instead of lists\n",
    "  * Lists are mutable and, therefore, not hashable\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = (\"John\", \"Doe\", \"32\", \"165,385\", 3)\n",
    "print(f\"The hash for data_1 is {hash(data_1)}\")\n",
    "\n",
    "data_2 = (\"Mat\", \"Doe\", \"32\", \"192,891\", 3)\n",
    "print(f\"The hash for data_2 is {hash(data_2)}\")\n",
    "\n",
    "data_3 = (\"Mark\", \"Smith\", \"34\", \"85,232\", 2)\n",
    "print(f\"The hash for data_3 is {hash(data_3)}\")\n",
    "\n",
    "data_4 = (\"Mindy\", \"Smith\", \"65\", \"160,000\", 42)\n",
    "print(f\"The hash for data_4 is {hash(data_4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Hashing and Proximity\n",
    "\n",
    "* We've shown that hashing can be much faster for finding identical items than a search\n",
    "\n",
    "* Unfortunately, hashing does not convey level of similarity (e.g.: using Jaccard) \n",
    "  * `data_1` and `data_2` are closer to each other than to `data_3`, but their hashes are not\n",
    "* Can we use hashing to convey some level of similarity?\n",
    "  * We could find potentially similar items using hashing (fast) and then compare the subset of items using Jaccard similarity?\n",
    "  * Pairwise comparison on a much smaller subset of data\n",
    "\n",
    "  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Hashing and Proximity - cont'd\n",
    "\n",
    "* Naive approach: normalize values to convey similarity\n",
    "\n",
    "1. Drop non-relevant unnecessary columns such as first and last name\n",
    "    * for instance, one could start by de-replicating highly-correlated variables.\n",
    "    * use domain knowledge to remove features that can be derived from other features\n",
    "2. Convert numerical values into bins\n",
    "3. Compare the entries over the converted data\n",
    "    * Compute the hash on some randomly picked subset of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Hashing and Proximity -1\n",
    "\n",
    "```python\n",
    "# original data\n",
    "data_1 = (\"John\", \"Doe\", \"32\", \"165,385\", 3)\n",
    "data_2 = (\"Mat\", \"Doe\", \"32\", \"192,891\", 3)\n",
    "data_3 = (\"Mark\", \"Smith\", \"34\", \"85,232\", 2)\n",
    "data_4 = (\"Mindy\", \"Smith\", \"65\", \"160,000\", 42)\n",
    "\n",
    "# Binned data\n",
    "\n",
    "data_1 = (\"milenial\", \"high\", \"low\")\n",
    "data_2 = (\"milenial\", \"high\", \"low\")\n",
    "data_3 = (\"millenial\", \"average\", \"low\")\n",
    "data_4 = (\"baby_boomer\", \"high\", \"high\")\n",
    "```\n",
    "\n",
    "* This approach supposes it's possible to bin the data into relevant categories\n",
    "  * Here we can create relative significant categories that may be satisfactory to find matching profiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [(\"milenial\", \"high\", \"low\"), (\"milenial\", \"high\", \"low\"), (\"millenial\", \"average\", \"low\"), (\"baby_boomer\", \"high\", \"high\")]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "[hash(i) for i in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Hashing and Proximity -2\n",
    "\n",
    "* When workign with large datasets (particulalry wide) and comparing the data across all the features may be CPU and disk intensive.\n",
    "\n",
    "* For example, image an isntance (singel entry) that has thousands of features\n",
    "  * Example 1: a simple way to numericize a documnt is by representing it a counts of its word occurrences\n",
    "  * Example 2: DNA testing companies can assay hundreds of thousands of genetic markers\n",
    "  * example 3: A medical expertiment, for each patient, one can record:\n",
    "      - Age\n",
    "      -  blood analysis:\n",
    "        - Complete blood count\n",
    "        - High Density Lipoprotein (HDL)\n",
    "        - Low Density Lipoprotein (LDL)\n",
    "        - White Blood Cell Count\n",
    "        - Red Blood Cell Count\n",
    "        - ...\n",
    "      - Immune system status\n",
    "        - Number of leukocytes\n",
    "        - Cytokine levels in serum\n",
    "        - ....\n",
    "      - Genetic background\n",
    "      - nb cigarettes in last month\n",
    "      - nb of alcoholic drinks in last month\n",
    "      - nb times used drugs in last month\n",
    "      - nb surgical operations in last year\n",
    "      - nb of medications taken\n",
    "      - nb of hospital visits\n",
    "      - ...\n",
    "  \n",
    "  \n",
    "  \n",
    "* It's dificult to compare data across all features \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Hashing and Proximity\n",
    "\n",
    "* Recall that the objective we defined above is to find items that are similar to a query $q$ \n",
    "* The proposed subset-bin approach is not a perfect replacement for pairwise comparisons but allows us to avoid unnecessary comparisons\n",
    "  * We only focus on pairs of items that generate matches \n",
    "* it's also ideal for dealing with very wide datasets\n",
    "   * In a dataset with thousands of features, we need only focus on a small subset of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Hashing Wide Datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* While this is computationally efficient, there is a risk that we might choose the wrong features\n",
    "    \n",
    "```python \n",
    "         [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "data_1 = [x, x, x, x, y, x, x, y]\n",
    "data_2 = [x, x, x, x, z, x, x, z]\n",
    "data_3 = [w, w, w, w, z, w, w, z]\n",
    "```\n",
    " * e.g.: randomly picking features 5 and 8 can miss identification of the closest item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Hashing Wide Datasets - cont'd\n",
    "\n",
    "* Perhaps we can repeat the algorithm multiple times\n",
    "\n",
    "1. Pick a random subset of features\n",
    "2. compute the hash on the selected subset of features\n",
    "3. Repeat a certain number of times\n",
    "\n",
    "* There are theoretical guarantees that can make this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Iterative Hashing\n",
    "\n",
    "* Given two datasets with $N$ features and $K$ features that match between $x_i$ and $x_j$\n",
    "* If we hash on $n$ features, we can compute the probability of a matching hash value\n",
    "  * We are selecting $n$ features and we want the probability that all $n$ features match\n",
    "\n",
    "\n",
    "![](https://www.dropbox.com/s/c7bbp98fhcn0s0q/select_jar.png?dl=0)\n",
    "\n",
    "$$\n",
    "p = \\frac{{K}\\choose{n}}{{N}\\choose{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "* This is a specific case of the hypergeometric distribution (https://en.wikipedia.org/wiki/Hypergeometric_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Iterative Hashing - Single iteration Probability\n",
    "\n",
    "e.g.: \n",
    "    \n",
    "* Suppose we have a dataset that has 100 features\n",
    "* Given two instances $x_i$ and $x_j$ that match over 95 of the features \n",
    "* what is the probability that x_i and x_j match on a randomly selected subset of 6 features?\n",
    "\n",
    "$$\n",
    "p = \\frac{{K}\\choose{n}}{{N}\\choose{n}} = \\frac{{95}\\choose{6}}{{100}\\choose{6}}\n",
    "$$\n",
    "\n",
    "* in Python\n",
    "\n",
    "```python\n",
    "math.comb(95, 6) / math.comb(100, 6) = 0.72908\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Iterative Hashing - Probability for Multiple iterations \n",
    "\n",
    "* The probability of two relatively similar instances matching may not be acceptable\n",
    "  * p=0.73 means that we may miss up to ~27% of the instances that are similar to a query\n",
    " \n",
    "* Repeating the process multiple times increases our chances of selecting $n$ features that match between $x_i$ and $x_j$\n",
    "\n",
    "\n",
    "\n",
    "* Specifically, given the probability of a single match (ex. p=0.73), repeating the process $n$ times means that the probability of at least one match is:\n",
    "\n",
    "$$\n",
    "1- (1-p)^{n}\n",
    "$$\n",
    "\n",
    "* This is based on the binomial probability distribution\n",
    "https://en.wikipedia.org/wiki/Binomial_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Iterative Hashing - Probability for Multiple iterations \n",
    "\n",
    "* The probability for a single perfect match in the example above is p=0.73\n",
    "\n",
    "* The probability of at least one match in 10 trials is $1 - (1-0.73)^{10}$\n",
    "\n",
    "* in Python\n",
    "\n",
    "```\n",
    "1 - (1-0.73)**10 = 0.9999979\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Simulating the probabilities.\n",
    "\n",
    "* A relatively tractable way to estimate probabilities for simple events is through simulation.\n",
    "\n",
    "E.g.: \n",
    "* Suppose we have a dataset that has 100 features \n",
    "* Given two instances $x_i$ and $x_j$ that match on 95 out of 100  features \n",
    "* What is the probability that x_i and x_j match on a randomly selected subset of 6 features?\n",
    "  * Randomly select 6 features a large number of times and compute the fraction of times you obtained a perfect match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intially x and y match\n",
    "x = [1 for i in  range(100)]\n",
    "y = [1 for i in  range(100)]\n",
    "print(f\"x is: {x}\\n\")\n",
    "print(f\"y is: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 positions where x and y will not match\n",
    "import random\n",
    "\n",
    "random_positions = random.sample(range(100), 5)\n",
    "random_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values in y at the selected positions\n",
    "for i in random_positions:\n",
    "    y[i] = 0\n",
    "    \n",
    "print(x)    \n",
    "print(y)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_columns = random.sample(range(100), 6)\n",
    "print(random_columns)\n",
    "values_for_x = tuple([x[random_columns[0]], x[random_columns[1]], x[random_columns[2]],  x[random_columns[3]], x[random_columns[4]], x[random_columns[5]]])\n",
    "values_for_y = tuple([y[random_columns[0]], y[random_columns[1]], y[random_columns[2]],  y[random_columns[3]], y[random_columns[4]], y[random_columns[5]]])\n",
    "print(values_for_x)\n",
    "print(values_for_y)\n",
    "print(hash(values_for_x))\n",
    "print(hash(values_for_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perfect_matches = 0\n",
    "\n",
    "for _ in range(100_000):\n",
    "    comparison_indices = random.sample(range(100), 6)\n",
    "    nb_matches = sum([x[i] == y[i] for i in comparison_indices])\n",
    "    if nb_matches == 6:\n",
    "        perfect_matches += 1 \n",
    "        \n",
    "print(perfect_matches/100_000)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can simulate an event of that has a probability of p \n",
    "# by simulating a coing flip\n",
    "\n",
    "random.choices([0, 1], weights=[0.27, 0.73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = []\n",
    "for i in range(10):\n",
    "    outcomes.append(random.choices([0, 1], weights=[0.27, 0.73])[0])\n",
    "print(outcomes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def has_a_success(prob):\n",
    "    outcomes = []\n",
    "    for i in range(10):\n",
    "        outcomes.append(random.choices([0, 1], weights=[1-prob, prob])[0])\n",
    "    if sum(outcomes) > 0:\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_a_success(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_a_success(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_successes = 0\n",
    "\n",
    "prob = 0.73\n",
    "\n",
    "for i in range(100_000):\n",
    "    if has_a_success(prob):\n",
    "        nb_successes +=1 \n",
    "        \n",
    "print(nb_successes/100_000)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Variables and Dimensionality\n",
    "\n",
    "* The approach above required \"binning\" the data to maximize matches. What\n",
    "* This approach works well when binning is possible or when the data is binary\n",
    "  * For example when working with presence-absence.\n",
    "* This solution is not practical when bin boundaries are not easy to derive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "### Data in Higher-Dimensional Space\n",
    "\n",
    "\n",
    "* Another approach for finding similar items requires thinking of the data in higher-dimensional space.\n",
    "\n",
    "* The data features are simply dimensions in space.\n",
    "  * The instances of a dataset with two features can be plotted in 2-D space. \n",
    "  * The instances of a dataset with three features can be plotted in 3-D space. \n",
    "  * The instances of a dataset with n features can be plotted in n-D space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Data in 2D\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/t0ca5t1qzkr635b/2d-data.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Data in 3D\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/4flfaojlfb2a101/3d-data.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Random Projections: An intuition\n",
    "\n",
    "* Instead of hashing the data, let's project it into a new line instead.\n",
    "\n",
    "    * Given some randomly selected line, project a point so that the projection is perpendicular to the projection line\n",
    "\n",
    "* Two point that are close in higher dimensional space *may* also be close on the line\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/pjmbski6b0v8s8y/point_3d.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Random Projections: An intution - Cont'd\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/vkmv5um7w256w33/3d_point_line.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Random Projections: An intution - Cont'd\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/irik02lzrkuykgp/projection_1.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Random Projections: An intution - Cont'd\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/8y35229n7sb66ep/projection_2.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Random Projections: An intution - Cont'd\n",
    "\n",
    "* Intuition: two points that are projected close to each other (say to the same line bin) are potentially similar and should be inspected further\n",
    "  * This is somewhat a relaxed version of hashing since instances don't need to hash to the same value \n",
    "     * Projecting close to each other in the same region is sufficient\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Projection Example - Cont'd\n",
    "\n",
    "* Exmaple in 2-D\n",
    "<img src=\"https://www.dropbox.com/s/9szk1c2p0bvvacp/projection_3.png?dl=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "### Random Projections: An intution - Cont'd\n",
    "<span style=\"color:lightgrey;\">\n",
    "    \n",
    "### Random Projections: An intution - Cont'd\n",
    "\n",
    "* Intuition: two points that are projected close to each other (say to the same line bin) are potentially similar and should be inspected further\n",
    "  * This is somewhat a relaxed version of hashing since instances don't need to hash to the same value \n",
    "    * Projecting close to each other in the same bin is sufficient\n",
    "    * We also don't need to bin the feature values\n",
    "</span>\n",
    "    \n",
    "\n",
    "* Since the lines are randomly selected two close points may still fall into separate bins\n",
    "* To compensate, we can repeat the process multiple times\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "### Projection Example \n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/irik02lzrkuykgp/projection_1.png?dl=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### Projection Example - Cont'd\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/8y35229n7sb66ep/projection_2.png?dl=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "###  Projection as a Dot Product\n",
    "\n",
    "\n",
    "* How do you project a point onto a new axis?\n",
    " \n",
    "*  The dot product (or inner product) of two vectors is the projection of one onto the line spanned by the other.\n",
    "\n",
    "* It describes the projected vector in terms of the reference vector\n",
    "  * After normalizing, we get a quantity that represents the magnitude of the projected vector in terms of the reference vector\n",
    "\n",
    "$$\n",
    "Proj_B~A = \\frac{A \\cdot B}{|B|}\n",
    "$$\n",
    "where $A \\cdot B$ is simply the dot product of A and B.\n",
    "\n",
    "\n",
    "$A \\cdot B = A_x \\times B_x + A_y \\times B_y$ \n",
    "\n",
    "and $|B|$ is the magnitude of |B|. This is needed to normalize the resulting quantity (express it in terms of vector B)\n",
    "\n",
    "$|B| = \\sqrt{B_x^2 + B_y^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# compute the normalized projection of A onto B.\n",
    "A = (3,4)\n",
    "B = (5,2)\n",
    "\n",
    "A_dot_B = A[0]*B[0] + A[1]*B[1]\n",
    "amp_B = math.sqrt(B[0]**2 + B[1]**2)\n",
    "print(f\"The magnitude of B is: {amp_B}\")\n",
    "proj = A_dot_B / amp_B\n",
    "print (f\"The magnitude of the projection  {proj}\")\n",
    "\n",
    "print(f\"The ratio of the projection in terms of B is {proj/amp_B}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (A_dot_B / amp_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Projection as a Dot Product\n",
    "\n",
    "A = (0.95, 8)\n",
    "B = (5,7)\n",
    "C = (6,5)\n",
    "D = (4,2)\n",
    "\n",
    "amp_D = math.sqrt(D[0]**2 + D[1]**2)\n",
    "print(f\"The magnitude of D is: {amp_D}\\n\")\n",
    "\n",
    "\n",
    "A_dot_D = A[0]*D[0] + A[1]*D[1]\n",
    "proj_A = A_dot_D / amp_D\n",
    "print (f\"The magnitude of the projection  {proj_A}\")\n",
    "print(f\"The ratio of the projection in terms of D is {proj_A/amp_D}\")\n",
    "print(f\"Projection occurs in bin {math.ceil(proj_A/amp_D)} \\n\")\n",
    "\n",
    "\n",
    "B_dot_D = B[0]*D[0] + B[1]*D[1]\n",
    "proj_B = B_dot_D / amp_D\n",
    "print (f\"The magnitude of the projection  {proj_B}\")\n",
    "print(f\"The ratio of the projection in terms of D is {proj_B/amp_D}\")\n",
    "print(f\"Projection occurs in bin {math.ceil(proj_B/amp_D)} \\n\")\n",
    "\n",
    "\n",
    "C_dot_D = C[0]*D[0] + B[1]*D[1]\n",
    "proj_C = C_dot_D / amp_D\n",
    "print (f\"The magnitude of the projection  {proj_C}\")\n",
    "print(f\"The ratio of the projection in terms of D is {proj_C/amp_D}\")\n",
    "print(f\"Projection occurs in bin {math.ceil(proj_C/amp_D)} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Formalism Behind Random Projections \n",
    "\n",
    "\n",
    "* The theory (Johnson-Lindenstrauss lemma) behind random projections guarantees approximate preservation of distance \n",
    "\n",
    "* We can project on a lower number of dimensions without distorting the distance between any two points by more than a factor of (1 $\\pm$ $\\varepsilon$)\n",
    "\n",
    "  * Where $\\varepsilon$ depends on the number of instances in the data\n",
    "\n",
    "https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Implementation Details\n",
    "\n",
    "  \n",
    "* If input is $n \\times d$ matrix $A$.\n",
    "\n",
    "* using an __appropriate__ a $d \\times k$ matrix $R$, we can define the projection of $A$ as:\n",
    "$$\n",
    "E = A R\n",
    "$$\n",
    "\n",
    "* Therefore, the matrix multiplication $A \\dot R$ projects each of our data points onto the random vectors in $\\mathbb{R}$.\n",
    "\n",
    "* Normalise by the vector's magnitude and take ceiling (or floor) of these values to get the bin each instance falls into\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example of Random projections in Action \n",
    "https://github.com/spotify/annoy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
