{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d28d55-2c38-4bd3-a1e8-f832ca45bbf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We start by importing SparkContext and creating a context\n",
    "# which we will use in class\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,2,3], [4,5,6], [7,8]]\n",
    "x.append([9,10])\n",
    "y = chain(*x)\n",
    "list(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9b0e668-69d8-4f64-809c-92c2b1c106bc",
     "showTitle": false,
     "title": ""
    },
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Core RDD Functions in Spark\n",
    "* Beyond basic functions like `map`, `flatmap`, `filter`, and `reduce`, Spark offers a variety of other essential RDD methods (both transformations and actions).\n",
    "* These functions enable easier and more efficient programming in the map-reduce paradigm.\n",
    "* Some transformative examples include:\n",
    "    * `distinct`: Yields an RDD with the unique elements from the source RDD.\n",
    "    * `union`: Combines two RDDs.\n",
    "    * `intersection`: Identifies common elements between two RDDs.\n",
    "    * `foreach`: Applies a lambda function to each RDD element without returning any value.\n",
    "      * Similar to map but without returning results.\n",
    "    * `cartesian`: Generates the cartesian product of the given RDD.\n",
    "    * And many more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93475416-fdbb-4e9c-9d70-9ec0378f584d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core RDD Functions in Spark - Cont'd\n",
    "\n",
    "* Spark also provides a range of actions for RDD, such as:\n",
    "    * `count`: Gives the total number of RDD elements.\n",
    "    * `sum`: Calculates the total sum of RDD elements (requires numeric data).\n",
    "    * `mean`: Determines the average of RDD elements (requires numeric data).\n",
    "    * `stats`: Returns comprehensive statistics, including count, mean, stdev, max, and min.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c3396f-f551-4793-bdcb-e7ced437a5ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Distinct example \n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\", \"A\", \"C\", \"D\"])\n",
    "dataset_1.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04c14459-4546-454c-9f1a-0214ada41af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b12f712e-91fc-4fd4-b849-0a5e32506ea5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3885f8ac-4762-4081-a6e6-2f330fae15bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Union example\n",
    "\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "both_datasets = dataset_1.union(dataset_2)\n",
    "\n",
    "both_datasets.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e948466-9b45-420f-84c3-586505add26b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# intersection example\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\", \"C\"])\n",
    "dataset_2 = sc.parallelize([\"N\", \"B\", \"C\", \"M\"])\n",
    "dataset_1.intersection(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b323fa7b-1795-4b67-bda6-afc2bf5fea68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da70402-83ff-47e9-a777-2a9c7ce4ae1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach example - what doesn't work!\n",
    "\n",
    "out = dataset_1.foreach(lambda x: print(x))\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d228f25f-665c-4f57-b187-7943a6a6e52b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc4d6363-c1f7-4b72-9f1b-7bcb6df5ad60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# foreach example - what works!\n",
    "dataset_1.foreach(lambda x: print(x + \"-suffix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22db1795-ab8d-44ce-bd51-ba8ff61e1372",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modified_rdd = dataset_1.map(lambda x: x + \"-suffix\")    \n",
    "modified_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6877b470-ce71-4e8c-8512-c085817d138d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cartesian example\n",
    "dataset_1 = sc.parallelize([\"A\", \"B\"])\n",
    "\n",
    "dataset_2 = sc.parallelize([\"D\", \"E\", \"F\"])\n",
    "\n",
    "dataset_1.cartesian(dataset_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a68269-0074-4a31-9815-6ab0a793648b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sum, mean axamples\n",
    "\n",
    "dataset_1 = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "(dataset_1.sum(), dataset_1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f748a67-6a35-4c2f-b84a-8a4644991861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# stats example\n",
    "\n",
    "dataset_1.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type([1, [1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6105e5-5bad-475e-be34-cb4bc32a72bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Essential RDD Functions for Tuples\n",
    "\n",
    "* Remember, key-value pairs (tuples) are a core component in the Map Reduce paradigm.\n",
    "\n",
    "  * e.g. `[(\"THE\", 12), (\"HI\", 2), (\"COURSE\", 2), (\"STUDENTS\", 3), ... ]`\n",
    "  * Spark offers a collection of methods tailored for these tuples, both as transformations and actions.\n",
    "\n",
    "* Transformations on Tuples:\n",
    "    * `sortByKey`: yields a new RDD ordered by its keys.\n",
    "    * `reduceByKey`: applies a function `f` to combine values by key, producing a new RDD.\n",
    "    * `groupByKey`: Creates a new RDD where values are assembled under their respective keys.\n",
    "    * `join`: Generates a new RDD by pairing values with matching keys from two datasets.\n",
    "        * Variants include: `leftOuter`, `rightOuter`, `fullOuter` joins.\n",
    "\n",
    "* Actions for Tuples:\n",
    "  * General actions like count are adaptable to any data type.\n",
    "  * To perform arithmetic actions on a tuple RDD, you can first transform it using map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854ab657-79ae-4060-9542-4771ba8890ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sortByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"C\", 12), (\"D\", 2), (\"A\", 2), (\"B\", 3)])\n",
    "data_1.sortByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(range(0,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1000000000000000000000000000000000000000000000000000000000000000000):\n",
    "    print(i)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e73a08f-6584-43e5-aa82-7652e48cd7dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# groupByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3), (\"A\", 5)])\n",
    "grouped_data = data_1.groupByKey().collect()\n",
    "grouped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c343e454-a954-4863-bab3-84502a720b2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for key, val in grouped_data:\n",
    "    print(f\"{key}\\t{list(val)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33760e11-66b5-40c2-a6ae-afc8a0ee2c28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reduceByKey example\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 12), (\"A\", 2), (\"C\", 2), (\"B\", 3), (\"C\", 3)])\n",
    "data_1.reduceByKey(lambda x,y: x+y).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72a93ec-61c1-434f-8ee2-9bf7f6c36bbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6), (\"D\", 11)          ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7),           (\"E\", 11)])\n",
    "\n",
    "data_1.join(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f76ad0-ab91-43aa-b41d-a1fb2ac31851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# leftOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.leftOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c228b00f-9670-486b-a532-525f9905744d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rightOuterJoin exmaple\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"D\", 3), (\"B\", 4), (\"C\", 6), ])\n",
    "data_2  = sc.parallelize([(\"A\", 2),           (\"B\", 5), (\"C\", 7), ])\n",
    "\n",
    "data_1.rightOuterJoin(data_2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d28264-a87a-4fdb-b225-899cdd0131f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# apply non tuple actions  by first using map\n",
    "# extract the values\n",
    "\n",
    "data_1  = sc.parallelize([(\"A\", 1), (\"A\", 3), (\"B\", 4), (\"C\", 6)])\n",
    "data_1.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a177eca1-5752-4539-9d85-27e509a2de45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Conclusion\n",
    "* An RDD (Resilient Distributed Dataset) is an immutable distributed collection of various data objects.\n",
    "  * This can include lines, tuples, JSON objects, and more.\n",
    "\n",
    "* RDDs are distributed across multiple nodes, enabling parallel operations through a low-level API.\n",
    "  * Any transformation on an RDD yields a new RDD.\n",
    "  * To explore the myriad of methods available for RDDs, use the `dir` function or consult documentation or cheatsheet.\n",
    "\n",
    "* Importantly, RDDs don't enforce a specific data structure.\n",
    "  * For instance, you can have:\n",
    "```python\n",
    "sc.parallelize([(\"A\", 1), {\"First\": \"John\", \"Salary\": 125_000}, (\"B\", 4), (\"C\", 6), ])\n",
    "```\n",
    "  * This is not a desired situation when working with structured data.\n",
    "\n",
    "* Spark also offers a data structure that enforces data validation and structure.\n",
    "   * This aids in optimizing data operations more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a913ea7b-1967-44f7-96c2-47e59be1d444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Dive Into Spark DataFrame\n",
    "* Spark DataFrames in pySpark represent immutable, distributed collections of data neatly organized into named columns.\n",
    "  * They can be visualized as tables in relational databases or analogous to Pandas's DataFrame.\n",
    "  * Facilitates querying either via SQL or Python-style syntax.\n",
    "\n",
    "* Example using SQL:\n",
    "\n",
    "```SQL\n",
    "session.sql(\"SELECT * from users WHERE age < 21\")\n",
    "```\n",
    "\n",
    "* Example using Python-style:\n",
    "\n",
    "```\n",
    "users.filter(users.age < 21)\n",
    "```\n",
    "\n",
    "* Given their structured nature, DataFrames enable enhanced optimizations internally.\n",
    "  * They can be created from various sources, including:\n",
    "    * Structured data files (json, csv, etc.)\n",
    "    * Parquet Files\n",
    "    * External databases\n",
    "    * Pre-existing RDDs\n",
    "* While RDDs are seen as collections of objects, DataFrames can be understood as collections of rows (instances).\n",
    "  * Similar to Pandas DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5d05d2-3700-4d05-b3d6-7249ba315933",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DataFrame Operations in Spark\n",
    "\n",
    "* Spark offers a variety of high-level functions tailored for DataFrames.\n",
    "  * Rooted in the map-reduce model, these functions address common tasks efficiently.\n",
    "  * While they serve as shortcuts, remember they're underpinned by core functions: map, flatmap, filter, and reduce.\n",
    "* While SparkContext is essential for RDDs, DataFrames rely on SparkSession.\n",
    "  * Tasks such as creating, registering, and executing SQL queries necessitate SparkSession.\n",
    "* Multiple methods are available to instantiate a DataFrame:\n",
    "  * From a csv: Each row is an object.\n",
    "  * From a json: Every record is as an object.\n",
    "    * Ensure every line contains a distinct, valid JSON object.\n",
    "      * This format used in assignment 1.\n",
    "    * From a text file: Here too, each row becomes an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "523aa088-460b-40e5-a097-138f6650e88a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Understanding Schemas\n",
    "* Schemas outline the data type structure of your fields.\n",
    "* They play an important role in Spark's optimization processes.\n",
    "* Schemas are essential in achieving the right in-memory compression.\n",
    "  * Post-compression, the data size in memory might be more compact than its uncompressed counterpart on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25c6f68-3b3b-41bb-9eb9-16172ef1b452",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed4311ea-5f45-42c2-96a1-5668ee79ce95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_df  = session.read.text('dbfs:/FileStore/pride_and_prejudice.txt')\n",
    "print(text_df.count())\n",
    "text_df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112e5725-2262-4e5c-8715-52a81adeda3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c88eea-8808-464e-9185-9bc5ad94a5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list(range(0,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb76197-b3cc-4d69-ba4f-ae69ce4ba43f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.csv(\"dbfs:/FileStore/flight_info.csv\", header=True)\n",
    "print(csv_df.count())\n",
    "csv_df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a95d4cf3-3ff5-494b-bb54-cdde66f1df43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f26b705f-c4b5-4e91-88c0-5dbd0b341f53",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* In Spark's StructField, the third parameter indicates whether the field is nullable or not.\n",
    "  * I.e., whether the field can be null or not.\n",
    "\n",
    "```python\n",
    "StructType(\n",
    "\tList(StructField(year,StringType,true),\n",
    "\t\tStructField(month,StringType,true),\n",
    "\t\tStructField(day,StringType,true),\n",
    "\t\tStructField(dep_time,StringType,true),\n",
    "\t\tStructField(dep_delay,StringType,true),\n",
    "\t\tStructField(arr_time,StringType,true),\n",
    "\t\tStructField(arr_delay,StringType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,StringType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,StringType,true),\n",
    "\t\tStructField(distance,StringType,true),\n",
    "\t\tStructField(hour,StringType,true),\n",
    "\t\tStructField(minute,StringType,true)\n",
    "\t)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4248de0-2b2b-4ac0-9b42-196915882650",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "csv_df = session.read.options(inferSchema = True).csv(\"dbfs:/FileStore/flight_info.csv\", header=True)\n",
    "\n",
    "print(csv_df.count())\n",
    "\n",
    "csv_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915f522c-115a-4fd7-b940-3e802ea10168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7c48dd0-ff4f-46b0-a426-eb0831d59fca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "StructType(\n",
    "\tList(\n",
    "\t\tStructField(year,IntegerType,true),\n",
    "\t\tStructField(month,IntegerType,true),\n",
    "\t\tStructField(day,IntegerType,true),\n",
    "\t\tStructField(dep_time,IntegerType,true),\n",
    "\t\tStructField(dep_delay,IntegerType,true),\n",
    "\t\tStructField(arr_time,IntegerType,true),\n",
    "\t\tStructField(arr_delay,IntegerType,true),\n",
    "\t\tStructField(carrier,StringType,true),\n",
    "\t\tStructField(tailnum,StringType,true),\n",
    "\t\tStructField(flight,IntegerType,true),\n",
    "\t\tStructField(origin,StringType,true),\n",
    "\t\tStructField(dest,StringType,true),\n",
    "\t\tStructField(air_time,IntegerType,true),\n",
    "\t\tStructField(distance,IntegerType,true),\n",
    "\t\tStructField(hour,IntegerType,true),\n",
    "\t\tStructField(minute,IntegerType,true)\n",
    "\t)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35a807f1-3f50-468c-8496-f92b86085704",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df  = session.read.json('dbfs:/FileStore/random_user_dicts.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35eaaf0c-93d2-4b7a-b948-1c3249258301",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3eb1207-1760-413b-b0af-3eaf1214350a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(json_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a10884-c12a-4ecd-a8ad-4095658d250c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef31580-3ff9-418f-9c54-f68b4df166af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.select(\"first_name\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9accd0cc-8b4a-44df-99c1-4d17d8754c90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "json_struct = StructType([\n",
    "    StructField(\"first_name\", StringType(), nullable=False, metadata=None),\n",
    "    StructField(\"last_name\", StringType(),  nullable=False, metadata=None),\n",
    "    StructField(\"lat_long\", \n",
    "                StructType([\n",
    "                    StructField(\"latitude\", FloatType(), metadata=None, nullable=True),\n",
    "                    StructField(\"longitude\", FloatType(), metadata=None, nullable=True)\n",
    "                ]), nullable=True, metadata=None),\n",
    "    StructField(\"state\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"user_id\", StringType(),  nullable=True, metadata=None),\n",
    "    StructField(\"zip\", StringType(),  nullable=True, metadata=None),    \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30e11c43-50ad-416f-8791-ad6c5f6626d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "dir(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2680f006-72b4-4228-808d-82aceafc1406",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import json\n",
    "\n",
    "json_df  = session.read.schema(json_struct).json('dbfs:/FileStore/random_user_dicts.json')\n",
    "print(json_df.count())\n",
    "json_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fe0beac-9ec6-481e-a29b-f3e11a643148",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note the lat_long field. It has its own format\n",
    "json_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5339d2f-1093-4b81-9bd0-c258c98d10da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Insights into Data Formats: Focus on Avro\n",
    "\n",
    "* For stable datasets, structured data formats like CSV/TSV are commonly used.\n",
    "  * However, data often evolves, affecting dataset structure over time.\n",
    "  * Avro, a row-based data serialization format, offers flexibility for evolving schemas.\n",
    "\n",
    "* Recall that Avro excels at handling complex data structures:\n",
    "  * Supports nested data and hierarchical structures efficiently.\n",
    "  * Example Avro schema for complex user data:\n",
    "```json\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"user_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"employed\", \"type\": [\"boolean\", \"null\"]},\n",
    "    {\"name\": \"salary\", \"type\": [\"long\", \"null\"]},\n",
    "    {\"name\": \"cars\", \"type\": {\"type\": \"array\", \"items\": \"string\"}},\n",
    "    {\"name\": \"children\", \"type\": {\n",
    "      \"type\": \"map\",\n",
    "      \"values\": {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Child\",\n",
    "        \"fields\": [\n",
    "          {\"name\": \"age\", \"type\": \"int\"},\n",
    "          {\"name\": \"school\", \"type\": \"string\"}\n",
    "        ]\n",
    "      }\n",
    "    }}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "* Avro advantages over JSON:\n",
    "  * Compact binary format: Reduces storage requirements and improves read/write performance.\n",
    "  * Schema evolution: Allows adding, removing, or changing fields without full data rewrite.\n",
    "  * Built-in compression: Supports various compression codecs for further storage optimization.\n",
    "\n",
    "* Avro in data processing:\n",
    "  * Natively supported by Apache Spark for efficient reading and writing.\n",
    "  * Enables schema-on-read, allowing for flexible data interpretation.\n",
    "  * Facilitates easy conversion to Spark DataFrames for SQL-like querying.\n",
    "\n",
    "* Considerations when using Avro:\n",
    "  * Requires schema management, but this ensures data consistency and enables validation.\n",
    "  * May require additional processing time for serialization/deserialization compared to raw text formats.\n",
    "  * Excellent for systems with frequent reads, as the schema is only transmitted once per file.\n",
    "\n",
    "* Apache Spark's integration with Avro allows for seamless processing of complex, evolving datasets while maintaining efficiency and schema flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a641f75-208e-4daa-814e-a59f124ecfe0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### SQL Capabilities in Apache Spark\n",
    "\n",
    "* SQL functionality is deeply integrated with Spark DataFrames\n",
    "    * Supports both DataFrame API and SQL syntax for data manipulation\n",
    "\n",
    "* SQL queries can be applied directly to DataFrames\n",
    "\n",
    "* Two main approaches for SQL-like operations in Spark:\n",
    "    1. DataFrame API methods (e.g., `.select()`, `.filter()`)\n",
    "    2. Spark SQL queries using `spark.sql()`\n",
    "\n",
    "* For complex queries or named references:\n",
    "    * Register a temporary view with `.createOrReplaceTempView()`\n",
    "    * Use `spark.sql()` method for querying\n",
    "\n",
    "* All queries (DataFrame API or SQL) benefit from optimization\n",
    "    * Utilizes the Catalyst optimizer for query planning and execution\n",
    "    * [Catalyst Optimizer Details](https://databricks.com/glossary/catalyst-optimizer)\n",
    "\n",
    "* Important to remember:\n",
    "    * DataFrame operations are immutable\n",
    "      * Each operation returns a new DataFrame\n",
    "    * To add or modify columns, create a new DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL with Spark DataFrames\n",
    "```python\n",
    "# Using DataFrame API\n",
    "result = df.select(\"column1\").filter(\"column2 > 10\")\n",
    "\n",
    "# Using SQL with temporary view\n",
    "df.createOrReplaceTempView(\"my_table\")\n",
    "result = spark.sql(\"SELECT column1 FROM my_table WHERE column2 > 10\")\n",
    "\n",
    "# Using SQL directly on DataFrame (Spark 3.0+)\n",
    "result = spark.sql(\"SELECT column1 FROM {df} WHERE column2 > 10\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Query Optimization\n",
    "\n",
    "* consider the following\n",
    "\n",
    "```python\n",
    "customers = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (3, \"Charlie\"),\n",
    "    (4, \"David\")\n",
    "], [\"customer_id\", \"name\"])\n",
    "\n",
    "orders = spark.createDataFrame([\n",
    "    (1, 100.0),\n",
    "    (2, 150.0),\n",
    "    (3, 200.0),\n",
    "    (1, 120.0),\n",
    "    (4, 80.0)\n",
    "], [\"customer_id\", \"amount\"])\n",
    "\n",
    "# Scenario 1: Join then Filter\n",
    "def join_then_filter():\n",
    "    result = customers.join(orders, \"customer_id\") \\\n",
    "                      .filter(col(\"amount\") > 100)\n",
    "\n",
    "# Scenario 2: Filter then Join\n",
    "def filter_then_join():\n",
    "    filtered_orders = orders.filter(col(\"amount\") > 100)\n",
    "    result = customers.join(filtered_orders, \"customer_id\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "642156da-3c50-40fe-844a-d9e826cf643d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Example of query optimization - cont'd\n",
    "![](https://www.dropbox.com/scl/fi/bnkxkbfrtotpvgfd5ybxd/unoptimized_optimized.png?rlkey=m4goppj4l2ny2r1g9uwippbh2&dl=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38a89b2f-3a1c-4441-942a-e3aa33a091bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.createTempView(\"users\")\n",
    "\n",
    "session.sql(\"\"\"\n",
    "SELECT first_name, COUNT(*)\n",
    "FROM users\n",
    "GROUP BY first_name; \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9032c62c-cfe4-40e2-a461-28e03bad0988",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "session.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM users\n",
    "WHERE first_name IN (\"Evan\", \"Sarah\", \"John\"); \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60b5559e-0114-417c-9795-79cbf3061e59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DataFrame and Select Queries\n",
    "\n",
    "* The same functionality is available using Python\n",
    "* Many additional functions, inlcuding analytics-specific ones are available through specific library\n",
    "\n",
    "    ```from pyspark.sql import functions as F```\n",
    "    * The functions as used with a select\n",
    "  * Can use `agg` to do specific operations and rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8344ba02-6ebc-4ca2-89ef-7791e425a6a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.filter(F.length(json_df.first_name) < 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3edc63b4-ece1-4b8b-834a-9750522df7df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.groupby(\"first_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d628fcc5-3a10-4f53-86a9-7814e0ebcec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "dir(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71111de0-d6fa-4952-8b75-70fc2b11fd41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_df.groupby(\"first_name\").agg(F.count(\"first_name\").alias('First Name Counts')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5556e816-74b0-41dc-b3fe-5af3ab85f049",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### More on Optimization\n",
    "\n",
    "1. **Caching Data**:\n",
    "   * Caching is a powerful optimization technique in Spark that can significantly improve performance for iterative algorithms or repeated queries.\n",
    "   * Cache table contents or query outputs to expedite repeated data access.\n",
    "2. **Spark's Tungsten compression**.\n",
    "  * Cached data, especially with Tungsten compression, can occupy less RAM compared to disk storage. \n",
    "    * It can make cached data occupy less memory than the original on-disk format.\n",
    "    * Employ Lazy Caching to cache data as needed and utilize `UNCACHE` to free up space for caching other DataFrames.\n",
    "\n",
    "4. **Adjusting DataFrame Partitioning**:\n",
    "   * Narrow Operations (e.g., `COUNT`): Operate independently on each partition, no data shuffle required.\n",
    "   * Wide Operations (e.g., `GROUPBY`): Require data shuffling as they need data from multiple partitions.\n",
    "   * Minimize data shuffling by tuning the number of partitions; excessive shuffling can lead to performance bottlenecks.\n",
    "\n",
    "5. **Additional Optimizations**:\n",
    "   * Explore further optimizations in Spark's [Performance Tuning documentation](https://spark.apache.org/docs/latest/sql-performance-tuning.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66b9221-cc6e-439c-8fc8-ebb124702f21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# session.catalog.clearCache()\n",
    "# df = session.range(0, 5_000_000).withColumn(\"square\", F.col(\"id\") * F.col(\"id\"))\n",
    "\n",
    "# def with_caching():\n",
    "#     df.cache()  \n",
    "#     df.count()  # I want to force caching to complete before moving to next chunk of code\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     for _ in range(10):  \n",
    "#         result1 = df.filter(F.col(\"square\") > 1000000).count()\n",
    "#         result2 = df.filter(F.col(\"square\") < 100).count()\n",
    "#     return time.time() - start_time\n",
    "\n",
    "# def without_caching():\n",
    "#     start_time = time.time()\n",
    "#     for _ in range(10):  \n",
    "#         result1 = df.filter(F.col(\"square\") > 1000000).count()\n",
    "#         result2 = df.filter(F.col(\"square\") < 100).count()\n",
    "#     return time.time() - start_time\n",
    "\n",
    "# # Run the tests\n",
    "# cache_time = with_caching()\n",
    "# session.catalog.clearCache()  \n",
    "# no_cache_time = without_caching()\n",
    "\n",
    "# print(f\"With caching: Time = {cache_time:.2f} seconds\")\n",
    "# print(f\"Without caching: Time = {no_cache_time:.2f} seconds\")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title: Optimizing Spark Partitioning: E-commerce Sales Analysis\n",
    "\n",
    "* Give an E-commerce website analyzing sales data: \n",
    "  * Large dataset with customer ID, product ID, sale amount, date\n",
    "  * Goal is to calculate total sales per customer\n",
    "\n",
    "* Default Partitioning:\n",
    "\n",
    "* 100 partitions across 10-node cluster\n",
    "  * Uneven customer distribution despite having the same number of rows.\n",
    "  * Significant data shuffling required\n",
    "\n",
    "* Optimized Partitioning:\n",
    "  * Repartition by customer ID: 1000 partitions\n",
    "  * Even distribution of customers per partition\n",
    "  * 100 partitions per node\n",
    "\n",
    "* Benefits:\n",
    "\n",
    "  * Reduced data shuffling\n",
    "  * Balanced workload across nodes\n",
    "  * Improved data locality and parallelism\n",
    "\n",
    "* Result:\n",
    "  * Faster analysis\n",
    "  * Efficient use of cluster resources\n",
    "  * Scalable solution for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_introduction_2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
