{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This practical uses the file `flight_info.csv`, which you can download from [this link](https://www.dropbox.com/scl/fi/jsbatbyh5hl8q187986v1/flight_info.csv?rlkey=8ky753wksm5e6zbxdhvcoz2c9&dl=1).\n",
    "\n",
    "**Important**: You must use Apache Spark for all tasks, even though this can technically run on a single machine. The goal is to practice using Spark in a way that simulates a distributed computing environment, so avoid any approach that wouldn't run on a single machine if the dataset was substantially larger. \n",
    "\n",
    "1. Read the file into a Spark DataFrame named `df`.\n",
    "   * Ensure the schema is inferred automatically, and indicate that the file includes a header row.\n",
    "\n",
    "2. Use any Spark method of your choice to display the first 10 rows of the DataFrame.\n",
    "\n",
    "3. How many unique carriers are listed in the file?\n",
    "\n",
    "4. How many flights are associated with each carrier?\n",
    "\n",
    "5. Which hour of the day has the most flight take offs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_practical",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
