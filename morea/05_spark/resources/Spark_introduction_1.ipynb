{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d14eef8e-7f19-4946-9d0f-20884122ba8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Scaling Up: Local Machine\n",
    "\n",
    "* Scaling up occurs within the same system hosting the data and running the computaiton\n",
    "  * Simple to carry out from a physical standpoint\n",
    "  * from a programmatic standpoint, it's managed by the operating system and programming libraries; does not require additional frameworks\n",
    "* Scabaility is typically limited by the OS physical resources on the system.\n",
    "  * RAM upper bound is determined by the OS  or the hardware(e.g., motherboard, number of RAM slots available, etc.)\n",
    "* The cost of a single machine at the highest configuration may be prohibitive\n",
    "* May not meet the demands of the workload at hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f127b161-c2c5-42de-bf9c-1d6aac2af7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Distributed Systems\n",
    "\n",
    "* The hardware specs may not be the same across machines, adding another layer of complexity if it doesn't\n",
    "<center>\n",
    "<img src=\"https://bc247.wordpress.com/wp-content/uploads/2014/11/network.jpg\" width=\"500\" height=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b4c13ab-adc4-4240-a9ed-0fb81aa7f829",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Key Requirements for Distributed Systems\n",
    "\n",
    "* A distributed system needs to meet several crucial criteria, including:\n",
    "\n",
    "  * Node Communication: Enabling seamless interaction among nodes in the network.\n",
    "  * Resilience to Faults: Ensuring that both data and operations remain unaffected by system failures.\n",
    "  * Scalability: The ability to scale computational resources to handle growing workloads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf345b2d-f0eb-4e0f-81bb-04098a0a293e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Apache Spark\n",
    "\n",
    "* Apache Spark is an open-source, distributed processing system used for big data workloads.\n",
    "  * Runs on a commodity cluster\n",
    "      *  Interconnected, off-the-shelf hardware (commodity hardware)\n",
    "\n",
    "* It's an enhancement to Hadoop's MapReduce\n",
    "  * Processes and retains data in memory for subsequent steps\n",
    "    * For smaller workloads, Sparkâ€™s data processing speeds are up to 100x faster than Hadoop's MapReduce\n",
    "\n",
    "* Written in Scala and runs in the JVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18f11d85-533e-4e25-90bf-9fcce5c39162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Apache Spark -- Cont'd\n",
    "* Designed for fast processing of large-scale data through distributed computing\n",
    "* Utilizes in-memory caching and optimized query execution for improved performance\n",
    "* Provides rich functionality:\n",
    "  * Over 80 high-level operators beyond Map and Reduce\n",
    "  * Tools for data pipeline construction and evaluation\n",
    "* Offers advantages over Hadoop MapReduce:\n",
    "  * More diverse set of operations and transformations\n",
    "  * Faster processing due to in-memory computation\n",
    "* Includes libraries to support:\n",
    "  * SQL queries (Spark SQL)\n",
    "    * Provides a wide range of functions for SQL-like operations, machine learning algorithms, and graph data processing\n",
    "\n",
    "  * Machine learning (MLlib)\n",
    "  * Graph data analysis (GraphX)\n",
    "  * Streaming data processing (Spark Streaming)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048bfbef-984c-4209-a67e-13d5d2c639ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### What is Apache Spark\n",
    "\n",
    "* [See Video](https://www.databricks.com/spark/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf96282e-53bf-4663-b90c-02918cfae9db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark and Functional Programming\n",
    "\n",
    "* To manipulate data, Spark uses functional programming\n",
    "  * The functional programming paradigm is used in many popular languages including Common Lisp, Scheme, Clojure, OCaml, and Haskell\n",
    "  \n",
    "* Functional programming is a data oriented paradigm\n",
    "  * Decomposes a problem into a set of functions.\n",
    "  * Logic is implemented by applying and composing functions.\n",
    "\n",
    "* The idea is that functions should be able to manipulate data without maintaining any external state.\n",
    "  * No external or global variables\n",
    " \n",
    "* In functional programming, we need to always return new data instead of manipulating the data in-place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef54bd37-5133-436b-8da7-7e302c6ebd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "\n",
    "What are RDDs? \n",
    " * Resilient Distributed Datasets is the data representation in Spark. \n",
    "   * an RDD is conceptually divided into one or more partitions. \n",
    "     Each partition is a self-contained unit of data that can be operated on independently and in parallel. \n",
    "     These partitions are distributed across the nodes in a Spark cluster for parallel computation.\n",
    "\n",
    "  * RDDs are read-only collections, partitioned across multiple computing nodes for optimized performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a53aff6-07df-4fc6-ace6-6a9a31351200",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Operations and RDDs: An Overview\n",
    "  * Each partition is also replicated across nodes.\n",
    "    * Number of replicates is a configuration parameter.\n",
    "  * Partitioning enhances fault tolerance and boosts the efficiency of data operations.\n",
    "  * This allows RDDs to be accessed through parallel operations\n",
    "    * Data operations can be executed on all partitions at the same time, speeding up data tasks.\n",
    "    \n",
    "* In-Memory Caching\n",
    "  * RDDs are stored in memory -- if the RDD can fit into a node's memory -- facilitating quick iterations over the same dataset.\n",
    "    * Disk Spilling: If an RDD is too large Spark spills what doesn't fit in RAM to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82a09076-0a99-4e0b-a59a-b5a1a9c89419",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components\n",
    "\n",
    "* Spark Core: The foundation of the entire Spark ecosystem.\n",
    "  * Defines the basic data structure for (RDDs).\n",
    "  * Provides a set of operations (transformations) and actions to process RDDs.\n",
    "  * Enables distributed data processing, fault tolerance, and in-memory computations.\n",
    "\n",
    "* Spark SQL: Spark's SQL engine for structured data.\n",
    "  * Supports ANSI SQL standards for query language.\n",
    "  * Transforms SQL queries into Spark operations.\n",
    "  * Allows for SQL-like querying on large datasets, bridging the gap between traditional databases and big data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21b3e958-4c48-4874-a693-aef5b3c487a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Overview of Apache Spark's Core Components - Cont'd\n",
    "\n",
    "* Spark Streaming: Real-time data processing module in Spark.\n",
    "  * Processes live streaming data.\n",
    "  * Offers seamless integration with other Spark components.\n",
    "  * Enables real-time analytics and data processing, vital for applications like fraud detection, monitoring, and recommendation systems.\n",
    "\n",
    "* MLlib: Spark's machine learning library.\n",
    "  * Implements machine learning algorithms on RDDs.\n",
    "  * Provides algorithms for classification, regression, clustering, and more.\n",
    "  * Allows for scalable machine learning tasks, leveraging Spark's distributed computing power.\n",
    "\n",
    "* GraphX: Spark's library for graph processing.\n",
    "  * Manages and manipulates graph structures.\n",
    "  * Performs parallel graph operations and computations.\n",
    "  * Enables graph analytics at scale, useful for social network analysis, recommendation systems, and more.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a83ee9cc-102d-4f0b-a3f8-ea04abe69c6f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Core Components of Spark\n",
    "<img src=\"https://www.dropbox.com/s/azebxe8nv5nsqne/spark_architecture.png?dl=1\" width=\"900\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cecd85f-8052-4735-9899-848545ba2573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark High-level Components\n",
    "\n",
    "* Cluster manager: Manages the resources across the Spark cluster.\n",
    "  * Responsible for allocating resources like CPU and memory to Spark applications.\n",
    "  \n",
    "* Application driver: The central orchestrator of the Spark program, housing the main application logic.\n",
    "  * Contains an isntance of the spark context\n",
    "    * Requests resources from the Cluster Manager to launch executors.\n",
    "    * Coordinates the overall data processing workflow.\n",
    "  \n",
    "* Executors (workers): These are the worker nodes to which tasks are delegated.\n",
    "  * They execute the code sent by the driver, specifically focusing on their designated partitions of the dataset.\n",
    "  * Executors communicate with the Cluster Manager to report status and failures.\n",
    "\n",
    "\n",
    "![](https://spark.apache.org/docs/latest/img/cluster-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c751e77-497a-4e72-b695-2e9894ce838e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Program Flow\n",
    "\n",
    "* A typical Spark program adheres to the following structure:\n",
    "  * Application Driver: Central point of the Spark program, where the main application logic resides. \n",
    "    * Responsible for coordinating the entire data processing workflow.\n",
    "      *  In client mode, the driver runs on the machine where the Spark job is submitted (master node).\n",
    "        * The professor is the driver  \n",
    "      *  In cluster mode, the driver runs on one of the worker nodes in the cluster.\n",
    "        * One of your peers is the driver, in addition to doing, or not doing, some of the work.  \n",
    "  * Executors (Workers): The worker nodes that tasks are delegated to.\n",
    "    * Executing the code sent by the driver, specifically focusing their designated partitions of the dataset.\n",
    "     * Driver send smaller, more specific operations that the executors can carry out.\n",
    "       * Referred to as a task plan.\n",
    "    * Result Aggregation: results sent by the executor to the application driver for aggregation\n",
    "      * Often a final layer of computation to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa78be3-1059-402b-9a4a-8aa64392708f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Application Lifecycle\n",
    "\n",
    "1. Python Program Initialization\n",
    "    * The `SparkContext` object is created; it's a client to interact with the Spark cluster.\n",
    "  \n",
    "2. Resource Request to Cluster Manager:\n",
    "    * `SparkContext` contacts the Cluster Manager to request resources (CPU, memory) for your application.\n",
    "  \n",
    "3. Cluster Manager Allocates Resources:\n",
    "    * The Cluster Manager allocates the necessary resources for the application and decides where to place the executors across the cluster's nodes.\n",
    "  \n",
    "4. Application Driver Initialization\n",
    "    * The main logic of your Spark application is in the Application Driver.\n",
    "    * It becomes the master node for your application, coordinating tasks between the cluster and your Python program.\n",
    "5. Executor Launch\n",
    "    * Based on the resources allocated by the Cluster Manager, executors for the Spark application are launched on the worker nodes.\n",
    "6. Task Division and Execution Plan\n",
    "    * The Application Driver divides the job into tasks and builds an execution plan.\n",
    "7. Sending Task Plans to Executors\n",
    "    * Instead of sending raw code, the Application Driver sends the execution plan (or task plans) to the Executors.\n",
    "8. Task Execution on Executors\n",
    "    * Executors run the tasks on their designated partition of the dataset.\n",
    "9. Result Aggregation\n",
    "    * After task execution, the Executors send the results back to the Application Driver.\n",
    "10. Final Computation and Output Retrieval\n",
    "    * The Application Driver may perform some final computations.\n",
    "    * Results are returned to your Python program through the `SparkContext` object.\n",
    "11. Resource Release\n",
    "    * Once the application completes, the resources are released back to the Cluster Manager for use by other applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71997d80-647d-4a9b-80d7-9eed2733ac03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Setting up a Docker Cluster\n",
    " \n",
    "* Installing Spark and all its components manually can be challenging and time-consuming.\n",
    "  * Configuring and optimizing a Spark cluster from scratch requires substantial effort.\n",
    "* Easier deployment options are available on cloud services, such as:\n",
    "  * [Amazon's EMR](https://aws.amazon.com/emr/features/spark/) for Spark support\n",
    "  * [Databricks' Community Edition](https://www.databricks.com/product/faq/community-edition) and paid offerings\n",
    "  * Other providers like Google's Dataproc, Microsoft's HDInsight, etc.\n",
    "\n",
    "* We'll utilize [Databricks Community Edition](https://community.cloud.databricks.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d13a01f5-e404-4c42-a6c7-cd1994c23166",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Installing Via Docker For ICS438 (Optional)\n",
    "\n",
    "* Note that the following is optional. All the work we will using pySpark will be done on a Databricks cluster in community edition. Also, this solutions assumes that you have docker running on your machine.\n",
    "\n",
    "* It is easy to use Docker to install locally. We will use the following Docker image\n",
    "  \n",
    "```  \n",
    "jupyter/all-spark-notebook\n",
    "```\n",
    "* There are other docker images, including (jupyter/pyspark-notebook), which does not include the jobs dashboard `http://localhost:4040`\n",
    "\n",
    "* We will run the infrastructure as follows:\n",
    "\n",
    "```\n",
    "docker run --rm -p 4040:4040 -p 8888:8888 -v $(pwd):/home/jovyan/work jupyter/all-spark-notebook\n",
    "```\n",
    "\n",
    "* This configuration created a master and compute nodes locally in a docker instance\n",
    " \n",
    "* While you're probably not going to need to, you can log into the running container using: `docker exec -it <CONTAINER_ID> bash`\n",
    "\n",
    " * where <CONTAINER_ID> of the container currently running the `jupyter/all-spark-notebook` image\n",
    "\n",
    "\n",
    "* The Docker instance has all the libraries installed and ready to go.\n",
    "\n",
    "* Make sure you run a Jupyer notebook on the Docker instnac\n",
    "  * If the code below fails, this means you're not running in the Docker instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "503e7db6-97e9-4c43-af09-c95fab6d7fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# pip install pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24d0ec7-0f4f-43ba-aa5b-e4c7b70fbc87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# help(SparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46de1c98-8b14-4a85-91f9-bfbf8b3b8d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Spark version is {sc.version}\")\n",
    "\n",
    "print(f\"Python version is {sc.pythonVer}\")\n",
    "\n",
    "print(f\"The name of the master is {sc.master}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67326381-b032-4b46-8659-92f9cf466937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b4617d-715e-46b2-9cb7-88bc37c8570b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Creating a Text RDD in Spark\n",
    "\n",
    "* Resilient Distributed Datasets (RDDs) can be created in various ways in Spark. Here are two commonly used methods:\n",
    "\n",
    "    * `parallelize()`: This function allows you to transform Python collections, such as lists or arrays, into an RDD.\n",
    "        * It distributes the elements of the passed collection across multiple nodes, making the RDD fault-tolerant.\n",
    "\n",
    "    * `textFile()`: This function reads in a text file and creates an RDD where each object corresponds to a line in the file.\n",
    "\n",
    "* Example using `parallelize()`:\n",
    "  ```python\n",
    "  from pyspark import SparkContext\n",
    "  sc = SparkContext()\n",
    "  my_list = [1, 2, 3, 4, 5]\n",
    "  my_rdd = sc.parallelize(my_list)\n",
    "  # or\n",
    "  my_text_rdd = sc.textFile(\"path/to/text/file.txt\")\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e953f692-1d30-4e0c-b4b8-4eb1c1545d04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fundamental Operations on RDDs\n",
    "\n",
    "* Several fundamental operations are available to manipulate and transform Spark RDDs. \n",
    "  * These operations are generally called 'transformations.'\n",
    "    * `map`: Applies a given function to each element of the RDD and returns a new RDD consisting of the results.\n",
    "    * `filter`: Returns a new RDD containing only the elements that satisfy a given predicate.\n",
    "    * `reduce`: Aggregates the elements of the RDD using a given function, \n",
    "      * The function should be commutative and associative so that it can be computed in parallel.\n",
    "\n",
    "* Each transformation on an RDD produces a new RDD without modifying the original one, making RDDs immutable.\n",
    "\n",
    "* `flatMap`: Another commonly used transformation, which first applies a function to all elements of the RDD and then flattens the results. It is conceptually equivalent to Python's `itertools.chain()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2257e1ca-bf9a-4faf-a06c-178a85510340",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
    "my_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027c111e-443d-4ec9-8011-6c469fac67e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99af2868-30db-4236-be93-746a67bec28b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ff42d0-be18-43c5-9387-40e423636c5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8052a8-d865-4d95-bc17-69a80168f634",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark's default behavior is sufficient and often near-optimal.\n",
    "partitions_data = my_rdd.glom().collect()\n",
    "partitions_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce025ac-2e6f-4284-b89b-73ae73b1bbd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "doubled_rdd = my_rdd.map(lambda x: x * 2)\n",
    "doubled_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ffc358-7e05-4980-ab72-fe122b8e4e1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "doubled_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eb69bb4-1ac9-4450-9621-b3d2f3e3cd8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "even_rdd = my_rdd.filter(lambda x: x % 2 == 0)\n",
    "even_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250e3f98-1c9a-49a8-b8f4-b65d6b885df4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66618f5-6761-4530-8d9a-ba7aa532c46b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum_of_elements = my_rdd.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ea0fde-7310-428d-b1f8-77ddc8338332",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sum_of_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c8f195b-fc6f-4465-897f-36dfc1b5ab63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapped_rdd = my_rdd.map(lambda x: (x, x * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc9f577-491e-46d3-91b3-1b4a2fa44db5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mapped_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f7cf592-b14b-448e-8c0b-4289a06603b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flat_rdd = my_rdd.flatMap(lambda x: (x, x * 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a980fa9-2805-405d-aee8-4d5ba4df0d43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flat_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b891cffa-a0f8-4183-a37c-c848c370a0be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Transformations and Actions in Spark\n",
    "\n",
    "* Transformations: These operations transform your data and produce a new RDD. They come in two types:\n",
    "    * Narrow-dependency transformations:\n",
    "        * Transformations where each partition of the parent RDD is used to build only one partition of the new RDD, e.g., `map` and `filter`.\n",
    "        * This means the operation can be performed independently on each partition, which allows for better parallelism and less data movement.\n",
    "    * Wide-dependency transformations:\n",
    "        * Transformations where a single partition of the parent RDD may be used to build multiple partitions of the child RDD, e.g., `groupBy` and `reduceByKey`.\n",
    "        * These operations are generally expensive in terms of performance as they typically require shuffling, or  redistributing, data across partitions\n",
    "\n",
    "* Actions: These operations trigger the computation and execute the job, producing a result.\n",
    "    * Each action initiates a Spark job, which may consist of multiple stages depending on the transformations involved.\n",
    "\n",
    "#### Why Distinguish Between Transformations and Actions?\n",
    "\n",
    "  * One reason is query optimization. For instance, performing a `filter` operation before a `groupBy` is usually more efficient than doing it afterward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bcc6c69-2122-43d5-aafa-91fedc5fb253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* Before Join\n",
    "\n",
    "* Partition 1 of RDD1: \n",
    "```\n",
    "(1, \"apple\")\n",
    "(2, \"banana\")\n",
    "```\n",
    "* Partition 2 of RDD1: \n",
    "```\n",
    "(3, \"cherry\")\n",
    "(4, \"date\")\n",
    "```\n",
    "  \n",
    "* Partition 1 of RDD2: \n",
    "```\n",
    "(3, \"red\")\n",
    "```\n",
    "* Partition 2 of RDD2: \n",
    "```\n",
    "(4, \"brown\")\n",
    "(2, \"yellow\")\n",
    "```\n",
    "\n",
    "Compute a jon between RDD1 and RDD2 based on the keys\n",
    "1 . shuffle the data around to group all occurrences of the same key together. \n",
    "* Data after shuffle\n",
    "\n",
    "  * Shuffle output targeting Partition 1 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (2, \"banana\")\n",
    "    (2, \"yellow\")\n",
    "    ```\n",
    "    \n",
    "  * Shuffle output targeting Partition 2 (from RDD1 and RDD2):\n",
    "    ```\n",
    "    (3, \"cherry\")\n",
    "    (3, \"red\")\n",
    "    (4, \"date\")\n",
    "    (4, \"brown\")\n",
    "    ```\n",
    "\n",
    "* Data Partitions After Join\n",
    "\n",
    "* Partition 1: \n",
    "```\n",
    "(2, (\"banana\", \"yellow\"))\n",
    "```\n",
    "* Partition 2\n",
    "```\n",
    "(3, (\"cherry\", \"red\")), (4, (\"date\", \"brown\"))\n",
    "```\n",
    "\n",
    "* What if we want to only join where key is 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f8a7e3b-c7b3-4c80-8c02-b7216216b444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Understanding the Concept of a Stage in Spark\n",
    "\n",
    "* A \"stage\" is a sequence of transformations that can be executed in parallel.\n",
    "  * I.e., narrow-dependency transformations.\n",
    "* Stages are separated by operations that require data to be rearranged\n",
    "  * I.e., wide dependencies.\n",
    "* Managing the complexity of operations in Spark is easier when tasks are grouped into stages.\n",
    "* Each stage either reads data, performs computations, or writes data.\n",
    "\n",
    "* Stages are executed in sequence, one after the other.\n",
    "  * Within each stage, tasks are executed in parallel.\n",
    "* Computation moves to where the data resides.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "140b46ff-e7c8-4e0c-882b-1496bd0265c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Decomposition into Stages\n",
    "\n",
    "```python\n",
    "flights_df = spark.read.option(\"head\", \"true\").option(\"inferSchema\", \"true\").csv(\"flights_info.csv\")\n",
    "flights_data_partitioned_df = flights_data.repartition(minPartitions=4)\n",
    "\n",
    "counts_df = flights_df.where(\"duration > 120\")\n",
    "                                       .select(\"dep\", \"dest\", \"carrier\", \"durations\")\n",
    "                                       .groupBy(\"carrier\")\n",
    "                                       .count()\n",
    "counts_df.collect()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c640c588-cb2f-495d-ab73-1a207cdac2cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Stages\n",
    "1. Reading Data\n",
    "* Reads the partitioned data into memory.\n",
    "  * A task for each partition.\n",
    "* This task has no dependency\n",
    "2. Filter, Select, and GroupBy\n",
    "* Applies .where(), .select(), and .groupBy().\n",
    "* Each task applies all these transformations on a partition.\n",
    "* GroupBy is \"Wide\" dependency (Needs to shuffle data between partitions for grouping)\n",
    "3. Count\n",
    "* Applies .count() to each group.\n",
    "* Each task calculates the count for groups in its partition.\n",
    "  * Each group is guaranteed to be in the same partition\n",
    "* This task has no dependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00631b18-045f-43e5-ac0b-e6c1f59848ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### PySpark: Job, Stages and Tasks\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/5qa1fb7p867i787/Page5.jpg?dl=1\" width=\"900\" height=\"600\">\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22c87cc-5fa2-497a-abd5-d945fbfe96f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install randomuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2d27bbd-6d2b-4359-86af-8ca61c661f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Insstall using the following if not already installed \n",
    "\n",
    "from randomuser import RandomUser\n",
    "\n",
    "# # Generate a single user\n",
    "user = RandomUser({\"nat\": \"us\"})\n",
    "print(f\"user object  is {user}\")\n",
    "def get_user_info(u):\n",
    "\n",
    "    user_dict = {\n",
    "        \"user_id\": u.get_id()[\"number\"], \n",
    "        \"first_name\": u.get_first_name(), \n",
    "        \"last_name\": u.get_last_name(), \n",
    "        \"state\": u.get_state(),\n",
    "        \"zip\": u.get_zipcode(),\n",
    "        \"lat_long\": u.get_coordinates()\n",
    "    }\n",
    "    return user_dict\n",
    "\n",
    "user_json = get_user_info(user)\n",
    "print(f\"user json representation  is\\n {user_json}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17eeb182-7d8f-49d2-a00a-a73b7e92a7c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "my_users = RandomUser.generate_users(500, {\"nat\": \"us\"})\n",
    "print(len(my_users))\n",
    "my_users[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a807114-710d-4c3f-ae16-a335257f7470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a list of 10 random users\n",
    "\n",
    "user_dicts = list(map(get_user_info, my_users))\n",
    "\n",
    "user_dicts[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6817cbf9-f86e-4171-a4a4-284a90f01ca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_rdd = sc.parallelize(user_dicts)\n",
    "users_rdd_size  = users_rdd.count()\n",
    "print(f\"The number of objects in my RDD is: {users_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5cfe97d-c065-453d-8ffb-91e55d5c562c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "users_rdd.takeSample(False, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c2c47f-ee8e-4046-b02e-33151f5b352a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select_users_rdd = users_rdd.filter(lambda x: x['state'] in [\"Hawaii\", \"Idaho\"])\n",
    "select_users_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be241d1f-fe5d-4e9a-b3ab-36df42156b2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# collect the result means grab them from all the chunk nodes\n",
    "select_users_rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630c8ced-716e-492b-86a3-85ab4786b5de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Building an RDD from a text file.\n",
    "text = sc.textFile('dbfs:/FileStore/pride_and_prejudice.txt', minPartitions=4)\n",
    "### Number of items in the RDD\n",
    "text.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caecd573-8bbe-4634-a2d7-286917452a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text_rdd_size = text.count()\n",
    "print(f\"number of objects in the RDD is {text_rdd_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e058ba-39fb-45ae-a27e-f359e82f4d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"/FileStore/pride_and_prejudice.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21364895-a949-46b5-a9bf-c7150ec21b5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "subset_x = text.take(10)\n",
    "print(f\"len of subset_x is: {len(subset_x)}\\n\")\n",
    "print(f\"type of subset_x is: {type(subset_x)}\\n\")\n",
    "print(f\"subset_x is:\\n{subset_x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e5e91d7-50ef-4964-b453-cf77470b371b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.map(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "378d00f7-542c-4a98-b78c-ad3fe02a5747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_split_line(line):\n",
    "    a = re.sub('\\d+', '', line)\n",
    "    b = re.sub('[\\W]+', ' ', a)\n",
    "    return b.upper().split()\n",
    "\n",
    "words = text.flatMap(clean_split_line)\n",
    "words.take(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29c2185e-9ea0-48c4-98a9-8a4b303c52aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0dc92a9-cfe2-498a-839a-5945925f6ea0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We want to do something like the following\n",
    "# words_mapped = words.map(lambda x: (x,1))\n",
    "\n",
    "words_mapped = words.map(lambda x: (x,1))\n",
    "words_mapped.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b063e48d-b739-4678-8705-220eff5e2a54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sorted_map = words_mapped.sortByKey()\n",
    "sorted_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2da4743b-d5bf-4c11-9073-b28c911ac012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sample = sorted_map.sample(withReplacement=False, fraction= 0.001)\n",
    "sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9d2a96-e1f1-42f0-aae6-7b967f9a9c8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "counts = words_mapped.reduceByKey(lambda x,y: x+y)\n",
    "counts.collect()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2e54ff0-687c-4e5d-82cc-344771c6fb36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As functional programming always returns new data instead of manipulating the data in-place, we can rewrite the above as:\n",
    "\n",
    "%%time\n",
    "counts_test_2 = text.flatMap(clean_split_line).map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "counts_test_2.take(100)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_introduction_1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
