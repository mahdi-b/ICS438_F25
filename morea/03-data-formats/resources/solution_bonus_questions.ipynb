{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Start running cells from here for the extra credit problems\n",
    "\n",
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a ref to the dataset.\n",
    "dataset = ds.dataset(\"s3://ursa-labs-taxi-data/\", partitioning=[\"year\", \"month\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print info for each fragment\n",
    "for fragment in fragments:\n",
    "    print(f\"Partition: {fragment.partition_expression}\")\n",
    "    print(f\"number of rows: {fragment.count_rows()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = 0\n",
    "\n",
    "for fragment in fragments:\n",
    "    row_count = fragment.count_rows()\n",
    "    total_count += row_count\n",
    "    \n",
    "    print(f\"Partition {fragment.partition_expression}: {row_count} rows\")\n",
    "\n",
    "print(f\"\\nTotal number of rows: {total_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "*** Can you get the average transaction between 2:00-2:59 PM? ***\n",
    "\n",
    "The idea here is to work smart, not hard. Instead of trying to handle all the data at once, we break it down into smaller, manageable pieces:\n",
    "\n",
    "We don't process all fragments at the same time. We don't even work with all row groups (smaller chunks within fragments) at once. \n",
    "\n",
    "Instead, we take it one step at a time, processing just one row group at a time. This approach has two main benefits:\n",
    "\n",
    "  * It's easier on your computer's memory. If a single row group is too big for your computer's RAM, you won't get stuck.\n",
    "  * It works great if you have multiple computers or processors. Each one can handle a different row group, speeding things up.\n",
    "  \n",
    "  * So, we can figure out how to compute the value on a single row group, and then iterate over all row groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frag = next(dataset.get_fragments())\n",
    "\n",
    "row_group_0 = first_frag.split_by_row_group()[0]\n",
    "\n",
    "# conver that single row group to a a table while making sure we only read only the relveant columns\n",
    "columns = ['pickup_at', 'total_amount']\n",
    "row_group_0_table = row_group_0.to_table(columns=columns)\n",
    "\n",
    "# Check the number of rows to make sure we've only read a single row group with two columns.\n",
    "# We can do any computaiton we desire on this row group\n",
    "\n",
    "row_group_0_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_group_0_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "from datetime import datetime, time\n",
    "\n",
    "pickup_at_column = row_group_0_table.column(\"pickup_at\")\n",
    "total_amount_column = row_group_0_table.column(\"total_amount\")\n",
    "\n",
    "total_amount = 0\n",
    "count = 0\n",
    "\n",
    "# We can covert the pick up field to a python time, which makes it easy to work with\n",
    "for i in range(len(pickup_at_column)):\n",
    "    timestamp = pickup_at_column[i].as_py()\n",
    "    transaction_time = timestamp.time()\n",
    "    if time(14, 0) <= transaction_time <= time(14, 59):\n",
    "        total_amount += total_amount_column[i].as_py()\n",
    "        count += 1\n",
    "\n",
    "# Calculate average transaction amount\n",
    "average_transaction_amount = total_amount / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_transaction_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can iterate over all fragments using:\n",
    "for i, row_group in enumerate(first_frag.split_by_row_group()):\n",
    "    print(f\"processing row group {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And naturally, to work on fragments, you can use\n",
    "for i, frag in enumerate(dataset.get_fragments()):\n",
    "    print(f\"processing frag {i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
