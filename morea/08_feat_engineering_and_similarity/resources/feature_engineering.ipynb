{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## The Challenge of Big Data in Document Analysis\n",
    "\n",
    "* Modern organizations face an explosion of textual data\n",
    "  * Emails, reports, social media, customer feedback, etc.\n",
    "* Extracting valuable insights from this data is crucial\n",
    "  * Improve decision-making, understand trends, enhance customer experience\n",
    "* Manual analysis is impractical due to sheer volume and the complexity\n",
    "* Need for efficient automated methods to process and analyze large document collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "### Some Key Tasks in Large-Scale Document Analysis\n",
    "\n",
    "* Identify important topics and themes across documents\n",
    "* Discover relationships between documents (e.g., similarity)\n",
    "* Classify documents into relevant categories\n",
    "* Enable efficient search and retrieval of relevant documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### The Challenge of Working with Text Data\n",
    "\n",
    "* Text is unstructured and not directly interpretable by computers\n",
    "  * Efficient analytics require structure\n",
    "  * Sophisticated analytics and machine learning algorithms operate on numbers, not words\n",
    "* Raw text lacks mathematical properties needed for many algorithms\n",
    "* Goal: Transform text into a format that preserves meaning and enables computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Why We Need Text Encoding\n",
    "\n",
    "* Enable mathematical operations on text\n",
    "  * Calculate similarity between documents\n",
    "  * Perform clustering and classification\n",
    "* Capture semantic relationships between words and documents\n",
    "* Reduce dimensionality and complexity of text data\n",
    "  * Allow for efficient processing of large document collections\n",
    "* Key challenge: Preserve important linguistic information during encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### From Words to Numbers: Text Representation Methods\n",
    "\n",
    "* Various techniques to convert text into numerical format:\n",
    "  * Bag of Words and TF-IDF: Based on word frequency\n",
    "  * Word Embeddings (e.g., Word2Vec): Capture semantic relationships\n",
    "  * Transoformer based embeddigns: use the popular transformer architecture to encode context\n",
    "  * Document Embeddings: Represent entire documents as vectors\n",
    "* Each method has its strengths and limitations\n",
    "* Choice of representation impacts downstream analysis tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Analyzing Large Document Collections\n",
    "\n",
    "* Identify common topics across documents\n",
    " * Focus on non-stop words for meaningful comparison\n",
    " * Use raw word counting to assess relationships\n",
    " * Simple approach, but has limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Challenges in Text Representation\n",
    "\n",
    "* Sparse Matches in Diverse Document Sets\n",
    "  * As document diversity increases, common content decreases\n",
    "  * Impacts similarity measures, clustering, and classification\n",
    "  * Challenge: Effectively represent documents\n",
    "    * Semantically similar doc are similar representation even if they shre little overlap\n",
    "\n",
    "* Can we just word counts as features? \n",
    "  * High Dimensionality: Using all non-stop words creates large, sparse datasets\n",
    "  * Similar words (e.g., \"leave,\" \"leaving,\" \"left\") counted separately\n",
    "  * Words can have multiple meanings based on context\n",
    "  * Example: \"bank\" in financial vs. geographical contexts\n",
    "  * In a vector representation, most entries would be zero or very small numbers, as most words appear rarely.\n",
    "  * This leads to high-dimensional, sparse vectors, which are computationally expensive to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Word Frequency Distributions\n",
    "\n",
    "* Words in natural language follow Zipf's law\n",
    "  * A few words occur very frequently, many words occur rarely\n",
    "  * Complicates statistical analysis and representation\n",
    "\n",
    "![Zipf's Law Distribution](https://www.dropbox.com/s/neydq8wi2kqqof3/zipf_law.png?dl=1)\n",
    "\n",
    "* Implications for text representation:\n",
    "  * Common words may not be most informative\n",
    "  * Rare words can be crucial but are statistically challenging\n",
    "  * Need for methods that balance frequency and importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Word Frequency Distributions\n",
    "\n",
    "* Let's say we analyzed a corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 50,000 times\n",
    "  * \"God\" appears 500 times\n",
    "  * \"Chthonic\" appears 20 times # word in relation of to inhibiting the underworld\n",
    "\n",
    "\n",
    "* Let's say we analyzed a second corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 48,950 times\n",
    "  * \"God\" appears 569 times\n",
    "  * \"Mammon\" appears 12 times   # word of Aramaic origin meaning riches and wealth\n",
    "\n",
    "* Let's say we analyzed a third corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 48,950 times  \n",
    "  * \"economy\" appears 569 times  \n",
    "  * \"Ricardian\" appears 12 times   # (Ricardian equivalence) therory related to the impact of Govt deficit ont the echonomy\n",
    "\n",
    "\n",
    "* Are corpora 1 and 2 the same?\n",
    "* It's hard to draw statistically significant conclusions about rare words due to their low frequency.\n",
    "* For instance, if \"serendipitous\" appears twice in one document and not at all in another, is this a meaningful difference or just chance?\n",
    "* Rare words might be very informative but are statistically unreliable due to their low frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding the Jaccard Coefficient\n",
    "\n",
    "* Jaccard Coefficient measures the overlap between two sets, A and B.\n",
    "  * It calculates the overlap by considering all the terms in both A and B.\n",
    "\n",
    "* It works even if A and B are of different sizes.\n",
    "\n",
    "* The result is always a value between 0 and 1.\n",
    "\n",
    "For two sets A and B, the Jaccard Coefficient J(A,B) is defined as:\n",
    "$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "Where:\n",
    "\n",
    "* $|A \\cap B|$ is the size of the intersection of sets A and B\n",
    "* $|A \\cup B|$ is the size of the union of sets A and B\n",
    "\n",
    "The Jaccard Coefficient always results in a value between 0 and 1, inclusive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding the Jaccard Coefficient - Cont'd\n",
    "\n",
    "* Limitations:\n",
    "  * It doesn't account for how often a term appears in the document\n",
    "    * It doesn't recognize that rare terms can be more valuable than common ones\n",
    "    * This is why simply looking at the intersection might not always be best.\n",
    "\n",
    "* A better method is needed to adjust for length, rather than just using $|A \\cup B|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Understanding Document Similarity: Measuring Relevance\n",
    "\n",
    "* Information Retrieval (IR) aims to rank documents by relevance to a search query\n",
    "  * Finding documents similar to search criteria\n",
    "  * Ranking documents based on closeness to search terms\n",
    "  * Prioritizing most relevant results for user convenience\n",
    "\n",
    "* Approach:\n",
    "  * Assign each document a score between 0 and 1\n",
    "  * Score represents alignment (relevance) with the search query\n",
    "  * Higher score indicates greater relevance\n",
    "\n",
    "* Evolution of techniques:\n",
    "  * Document Retrieval field has developed numerous innovative solutions\n",
    "  * Continuous improvement in accuracy and efficiency of matching algorithms\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Measuring Document Similarity: Calculating a Match Score\n",
    "\n",
    "* Basic principle of term-based scoring:\n",
    "  * Single-term search as a simple example\n",
    "  * Score of 0 if search term is absent from document\n",
    "  * Score increases with frequency of search term in document\n",
    "\n",
    "* Key factors in scoring:\n",
    "  * Presence/absence of search terms\n",
    "  * Frequency of term occurrence\n",
    "  * (Implied) More complex scoring for multi-term searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term-Document Count Matrices\n",
    "* A count matrix displays the frequency of each word within a document.\n",
    "  * This approach is known as the \"bag of words\" model.\n",
    "* The sequence of words in the document is not taken into account.\n",
    "* For instance, the phrases \"John is quicker than Mary\" and \"Mary is quicker than John\" would produce identical vectors in this model.\n",
    "* Having precomputed count matrices makes it easy to work with large datasets, as we don't need to preprocess text each time we analyze it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term Frequency (tf)\n",
    "\n",
    "* Term frequency, denoted as $tf_{t,d}$, measures how often a term $t$ appears in a document $d$.\n",
    "\n",
    "* While a higher $tf$ often suggests a better match, it doesn't always directly correspond to the significance of that match:\n",
    "\n",
    "  * A document where a term appears 10 times is likely more relevant than one where it appears only once.\n",
    "  \n",
    "  * However, it's not necessarily 10 times more relevant.\n",
    "\n",
    "* This indicates that relevance doesn't scale linearly with term frequency.\n",
    "\n",
    "* The $tf$ measure helps quantify the importance of a term within a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's a revised and cleaned-up version of the slide:\n",
    "\n",
    "### Understanding Log-Frequency Weighting\n",
    "\n",
    "* The weight of term $t$ in document $d$ is calculated using the logarithmically scaled frequency:\n",
    "\n",
    " $tf{t,d} = log (1 + f{t,d})$\n",
    "\n",
    "\n",
    "* This logarithmic scaling helps moderate the impact of high-frequency terms:\n",
    "  * $f_{t,d} = 0 \\rightarrow tf_{t,d} = 0$\n",
    "  * $f_{t,d} = 1 \\rightarrow tf_{t,d} \\approx 0.30$\n",
    "  * $f_{t,d} = 2 \\rightarrow tf_{t,d} \\approx 0.48$\n",
    "  * $f_{t,d} = 9 \\rightarrow tf_{t,d} = 1$\n",
    "  * $f_{t,d} = 1000 \\rightarrow tf_{t,d} = 3$\n",
    "\n",
    "* While variations exist, this approach captures the core idea of log-frequency weighting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importance of Document Frequency\n",
    "\n",
    "* The challenge of rare terms remains:\n",
    "  * Rare terms often provide more valuable information than common ones.\n",
    "    * Think of stop words as an example.\n",
    "* Take the term 'arachnid' in a query, which is seldom found in the collection:\n",
    "  * A document that includes this term is highly probable to be pertinent to the query 'arachnid'.\n",
    "  * This term significantly aids in contrasting documents effectively.\n",
    "  * Therefore, it's beneficial to assign a higher weight to infrequent terms like 'arachnid'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuing with Document Frequency\n",
    "\n",
    "* Highly common terms often offer less unique information than their rarer counterparts.\n",
    "* Think of a query term that's widely seen in the collection, like `high`, `increase`, or `because`:\n",
    "  * Solely using the $tf$ score, a document with these terms seems more relevant compared to one without.\n",
    "  * However, this doesn't guarantee its significance.\n",
    "* To evaluate how often a term appears across documents, we'll determine (or normalize using) its document frequency, denoted as `df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse Document Frequency (`idf`)\n",
    "- $N$ is the total number of documents $D$\n",
    "    -  $N = |D|$\n",
    "- $\\mbox{df}_t$ is the number of documents that contain the term t.\n",
    "  - This is used as a measure of how common or A term is across all documents.\n",
    "  * Note: $\\mbox{df}_t \\le N$, with $N$ being the entire document count.\n",
    "\n",
    "- The inverse document frequency ($\\mbox{idf}$) of term $t$ is defined as:\n",
    "$$\n",
    "idf_{t,D} = log_{10}(N/\\mbox{df}_{t,D})\n",
    "$$\n",
    "\n",
    "- We opt for the inverse since it's more practical than handling minuscule numbers, especially when $N$ is much larger than $\\mbox{df}_t$.\n",
    "- The logarithm (`log`) is incorporated to temper the `idf` effect, which becomes especially vital when managing vast document sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (`tf-idf`) Scheme\n",
    "\n",
    "\n",
    "- The `tf-idf` weight of a term is derived from multiplying its term frequency (`tf`) and inverse document frequency (`idf`).\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "tf.idf &= \\mbox{tf}_{t,d} \\times \\mbox{idf}_{t,D} \\\\\n",
    "&= log(1+\\mbox{tf}_{t,d}) \\times log(N/\\mbox{df}_{t,D})\n",
    "\\end{split}\n",
    "$$\n",
    "- This weighting method is a well-accepted strategy in the realm of information retrieval.\n",
    "\n",
    "- The weight:\n",
    "  * Rises as a term's occurrence within a document increases.\n",
    "  * Also grows with the term's scarcity across the entire document set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Score for a Document Given a Query\n",
    "\n",
    "\n",
    "$$\n",
    "Score(Q, T) = \\sum_{t\\in Q\\cap T} \\mbox{tf}.\\mbox{idf}_{t,d}\n",
    "$$\n",
    "\n",
    "* There are other variants of the tf.id\n",
    "  * Covered at the end \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `tf-idf` for Feature Engineering\n",
    "* Each document is represented by a real-valued vector of $\\mbox{tf-idf}$ weights $\\in R^{|V|}$\n",
    "\n",
    "![](https://www.dropbox.com/s/1bx77e488ee6wek/count_tf_idf.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "docs = {\n",
    "    \"doc_1\": \"the sky is blue\",\n",
    "    \"doc_2\": \"the sun is bright\",\n",
    "    \"doc_3\": \"the sun in the sky is bright\",\n",
    "}\n",
    "\n",
    "docs = {k: v.lower().split() for k, v in docs.items()}\n",
    "print(f\"There are {len(docs)} documents\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "\n",
    "def compute_tf(term, document):\n",
    "    return math.log10(1 + document.count(term))\n",
    "\n",
    "compute_tf(\"the\",  docs['doc_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tf(\"the\",  docs['doc_3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(term, documents):\n",
    "    N = len(documents)\n",
    "    df = sum(1 for document in documents if term in document)    \n",
    "    return math.log10(N / df)\n",
    "\n",
    "compute_idf(\"the\", docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_idf(\"blue\", docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(term, document, documents):\n",
    "    tf = compute_tf(term, document)\n",
    "    idf = compute_idf(term, documents)\n",
    "    return round(tf * idf, 2)\n",
    "\n",
    "compute_tf_idf(\"the\",  docs[\"doc_1\"], docs.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tf_idf(\"sky\",  docs[\"doc_1\"], docs.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_tf_idf(\"blue\",  docs[\"doc_1\"], docs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Representation in Information Retrieval (IR) using TF-IDF\n",
    "\n",
    "1. Vector Representation:\n",
    "   - Represent both queries and documents as TF-IDF vectors in a high-dimensional space\n",
    "   - Each dimension corresponds to a unique term in the corpus\n",
    "   - The value for each dimension is the TF-IDF score of that term\n",
    "\n",
    "2. Document Ranking:\n",
    "   - Rank documents based on their similarity to the query vector in this TF-IDF space\n",
    "   - Similarity is measured by how \"close\" document vectors are to the query vector\n",
    "   - Raw Euclidean distance can be misleading, especially with varying vector lengths:\n",
    "     * Longer documents (with more terms) might have larger TF-IDF vectors, leading to greater Euclidean distances even if content is relevant.\n",
    "\n",
    "3. Angle-Based Ranking:\n",
    "   - Instead of Euclidean distance, rank documents based on the angle between their TF-IDF vector and the query's TF-IDF vector\n",
    "     - less affected by vector length, focusing more on the direction (content) than magnitude (document length)\n",
    "\n",
    "4. Advantages of TF-IDF in IR:\n",
    "   - Balances the importance of term frequency within a document (TF) and term uniqueness across documents (IDF)\n",
    "   - Helps to reduce the impact of common, less informative words (like \"the\", \"is\") which would have high TF but low IDF\n",
    "   - Enhances the importance of terms that are frequent in a document but rare across the corpus, likely indicating relevance\n",
    "\n",
    "This approach allows for effective comparison of document relevance to queries, taking into account both the frequency of terms and their importance in distinguishing between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "doc_1 = \"\"\"The king hath happily received, Macbeth, The news of thy success; and when he reads. Thy personal \n",
    "venture in the rebels' fight, His wonders and his praises do contend Which should  be thine or his: silenced \n",
    "with that, In viewing o'er the rest o' the selfsame day, He finds thee in the stout Norweyan ranks, Nothing\n",
    "afeard of what thyself didst make, Strange images of death. As thick as hail Came post with post; and every\n",
    "one did bear Thy praises in his kingdom's great defence, And pour'd them down before him.\"\"\"\n",
    "\n",
    "doc_1 = doc_1.lower()\n",
    "doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_2 =  \"\"\"The king was really happy to hear about your success, Macbeth. \n",
    "When he read about your brave actions fighting the rebels, he was so impressed he couldn't\n",
    "decide who to praise more, you or himself. That same day, he noticed you also stood bravely\n",
    "against the Norwegians and not scared at all, even when facing dangerous situations. Many\n",
    "messengers came, one after the other, all of them talking about how great you were in defending\n",
    "the kingdom. They all praised you in front of the king.\"\"\"\n",
    "\n",
    "doc_2 = doc_2.lower()\n",
    "doc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor \n",
    "incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat. Ut  enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat.\"\"\"\n",
    "lorem_ipsum = lorem_ipsum.lower()\n",
    "lorem_ipsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [doc_1, doc_2,  doc_1 + lorem_ipsum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\\n\".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['king', 'happily', 'and', \"thy\", \"ipsum\", \"laboris\"]\n",
    "print([compute_tf_idf(v,  corpus[0], corpus) for v in vocabulary])\n",
    "print([compute_tf_idf(v,  corpus[1], corpus) for v in vocabulary])\n",
    "print([compute_tf_idf(v,  corpus[2], corpus) for v in vocabulary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "[corpus[0].lower().split().count(v) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(vocabulary=vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec.fit_transform(corpus).todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def tokenize(document):\n",
    "    # Tokenizing using a regex to match words and convert to lowercase\n",
    "    return re.findall(r'\\w+', document.lower())\n",
    "\n",
    "def compute_tf(term, document):\n",
    "    words = tokenize(document)\n",
    "    term_count = words.count(term)\n",
    "    total_terms = len(words)\n",
    "    return term_count / total_terms if total_terms > 0 else 0\n",
    "\n",
    "def compute_idf(term, documents):\n",
    "    N = len(documents)\n",
    "    df = sum(1 for document in documents if term in tokenize(document))\n",
    "    return math.log((1 + N) / (1 + df)) + 1\n",
    "\n",
    "def compute_tf_idf(document, terms, idfs):\n",
    "    tf_idf_raw = [compute_tf(term, document) * idfs.get(term, 0) for term in terms]\n",
    "    norm = np.linalg.norm(tf_idf_raw, 2)\n",
    "    tf_idf_normalized = [np.round(value / norm, 2) if norm > 0 else 0 for value in tf_idf_raw]\n",
    "    return tf_idf_normalized\n",
    "\n",
    "# Assuming vocabulary and corpus are defined\n",
    "idfs = {term: compute_idf(term, corpus) for term in vocabulary}\n",
    "\n",
    "for doc in corpus:\n",
    "    tf_idf_values = compute_tf_idf(doc, vocabulary, idfs)\n",
    "    print(tf_idf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "X = c_vec.fit_transform(corpus).toarray()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### `tf-idf` Weighing  Variants\n",
    "\n",
    "* Just an FYI\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/r88cmbmaqyk7hcp/weighting_schemes.png?dl=1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "</div>\n",
    "1. Term frequency, Document frequency, and Normalization.\n",
    "\n",
    "2. Term frequency (tf) has several variants:\n",
    "   - n (natural): Just the raw count of a term in a document.\n",
    "   - l (logarithm): 1 + log of the term frequency.\n",
    "   - a (augmented): A formula that prevents bias towards longer documents.\n",
    "   - b (boolean): 1 if the term appears, 0 if it doesn't.\n",
    "   - L (log average): A more complex logarithmic formula.\n",
    "\n",
    "3. Document frequency (df) has three variants:\n",
    "   - n (no): Just 1, meaning no weighting is applied.\n",
    "   - t (idf): The classic inverse document frequency formula.\n",
    "   - p (prob idf): A probabilistic version of idf.\n",
    "\n",
    "4. Normalization also has several options:\n",
    "   - n (none): No normalization.\n",
    "   - c (cosine): Cosine normalization.\n",
    "   - u (pivoted unique): Normalization based on unique terms.\n",
    "   - b (byte size): Normalization based on document length.\n",
    "\n",
    "5. THe SMART notation is a way to concisely represent different combinations of these weighting schemes. It uses a six-letter code (ddd.qqq), where:\n",
    "   - The first three letters (ddd) represent the weighting for the collection document.\n",
    "   - The second three letters (qqq) represent the weighting for the query document.\n",
    "\n",
    "For example, \"ltc.lnn\" would mean:\n",
    "- For the collection document: l (log tf), t (idf), c (cosine normalization)\n",
    "- For the query document: l (log tf), n (no df weighting), n (no normalization)\n",
    "\n",
    "This notation allows researchers and practitioners to quickly communicate which specific combination of weighting schemes they're using in their information retrieval system.\n",
    "  \n",
    "* See [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) for more info if you're interested in the topic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
