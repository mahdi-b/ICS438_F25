{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd090ea1-b0c5-4475-bf97-fd02c4424d0c",
   "metadata": {},
   "source": [
    "## The Challenge of Big Data in Document Analysis\n",
    "\n",
    "* Modern organizations face an explosion of textual data\n",
    "  * Emails, reports, social media, customer feedback, etc.\n",
    "* Extracting valuable insights from this data is crucial\n",
    "  * Improve decision-making, understand trends, enhance customer experience\n",
    "* Manual analysis is impractical due to sheer volume and the complexity\n",
    "* Need for efficient automated methods to process and analyze large document collections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f295076c-189d-4220-a072-2824c59c9b16",
   "metadata": {},
   "source": [
    "\n",
    "### Some Key Tasks in Large-Scale Document Analysis\n",
    "\n",
    "* Identify important topics and themes across documents\n",
    "* Discover relationships between documents (e.g., similarity)\n",
    "* Classify documents into relevant categories\n",
    "* Enable efficient search and retrieval of relevant documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0d604-7ae8-4715-a41e-0ca30f1f1fbd",
   "metadata": {},
   "source": [
    "### The Challenge of Working with Text Data\n",
    "\n",
    "* Text is unstructured and not directly interpretable by computers\n",
    "  * Efficient analytics require structure\n",
    "  * Sophisticated analytics and machine learning algorithms operate on numbers, not words\n",
    "* Raw text lacks mathematical properties needed for many algorithms\n",
    "* Goal: Transform text into a format that preserves meaning and enables computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656412d0-6529-447b-8167-987415dd649c",
   "metadata": {},
   "source": [
    "### Why We Need Text Encoding\n",
    "\n",
    "* Enable mathematical operations on text\n",
    "  * Calculate similarity between documents\n",
    "  * Perform clustering and classification\n",
    "* Capture semantic relationships between words and documents\n",
    "* Reduce dimensionality and complexity of text data\n",
    "  * Allow for efficient processing of large document collections\n",
    "* Key challenge: Preserve important linguistic information during encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17326a15-5106-482b-8f0c-139fc7ab2e06",
   "metadata": {},
   "source": [
    "### From Words to Numbers: Text Representation Methods\n",
    "\n",
    "* Various techniques to convert text into numerical format:\n",
    "  * Bag of Words and TF-IDF: Based on word frequency\n",
    "  * Word Embeddings (e.g., Word2Vec): Capture semantic relationships\n",
    "  * Transoformer based embeddigns: use the popular transformer architecture to encode context\n",
    "  * Document Embeddings: Represent entire documents as vectors\n",
    "* Each method has its strengths and limitations\n",
    "* Choice of representation impacts downstream analysis tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c7eb0c-63e9-4908-a108-2c4f16fbd8a2",
   "metadata": {},
   "source": [
    "### Analyzing Large Document Collections\n",
    "\n",
    "* Identify common topics across documents\n",
    " * Focus on non-stop words for meaningful comparison\n",
    " * Use raw word counting to assess relationships\n",
    " * Simple approach, but has limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae11b8f-1ea6-4890-bc2b-111dc788c308",
   "metadata": {},
   "source": [
    "### Challenges in Text Representation\n",
    "\n",
    "* Sparse Matches in Diverse Document Sets\n",
    "  * As document diversity increases, common content decreases\n",
    "  * Impacts similarity measures, clustering, and classification\n",
    "  * Challenge: Effectively represent documents\n",
    "    * Semantically similar doc are similar representation even if they shre little overlap\n",
    "\n",
    "* Can we just word counts as features? \n",
    "  * High Dimensionality: Using all non-stop words creates large, sparse datasets\n",
    "  * Similar words (e.g., \"leave,\" \"leaving,\" \"left\") counted separately\n",
    "  * Words can have multiple meanings based on context\n",
    "  * Example: \"bank\" in financial vs. geographical contexts\n",
    "  * In a vector representation, most entries would be zero or very small numbers, as most words appear rarely.\n",
    "  * This leads to high-dimensional, sparse vectors, which are computationally expensive to process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890cf84a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Word Frequency Distributions\n",
    "\n",
    "* Words in natural language follow Zipf's law\n",
    "  * A few words occur very frequently, many words occur rarely\n",
    "  * Complicates statistical analysis and representation\n",
    "\n",
    "![Zipf's Law Distribution](https://www.dropbox.com/s/neydq8wi2kqqof3/zipf_law.png?dl=1)\n",
    "\n",
    "* Implications for text representation:\n",
    "  * Common words may not be most informative\n",
    "  * Rare words can be crucial but are statistically challenging\n",
    "  * Need for methods that balance frequency and importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aaa9fb-306a-4253-838d-2f62c15d5d0a",
   "metadata": {},
   "source": [
    "### Word Frequency Distributions\n",
    "\n",
    "* Let's say we analyzed a corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 50,000 times\n",
    "  * \"God\" appears 500 times\n",
    "  * \"Chthonic\" appears 20 times # word in relation of to inhibiting the underworld\n",
    "\n",
    "\n",
    "* Let's say we analyzed a second corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 48,950 times\n",
    "  * \"God\" appears 569 times\n",
    "  * \"Mammon\" appears 12 times   # word of Aramaic origin meaning riches and wealth\n",
    "\n",
    "* Let's say we analyzed a third corpus of 1,000,000 words and found:\n",
    "  * \"the\" appears 48,950 times  \n",
    "  * \"economy\" appears 569 times  \n",
    "  * \"Ricardian\" appears 12 times   # (Ricardian equivalence) therory related to the impact of Govt deficit ont the echonomy\n",
    "\n",
    "\n",
    "* Are corpora 1 and 2 the same?\n",
    "* It's hard to draw statistically significant conclusions about rare words due to their low frequency.\n",
    "* For instance, if \"serendipitous\" appears twice in one document and not at all in another, is this a meaningful difference or just chance?\n",
    "* Rare words might be very informative but are statistically unreliable due to their low frequency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723aaa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding the Jaccard Coefficient\n",
    "\n",
    "* Jaccard Coefficient measures the overlap between two sets, A and B.\n",
    "  * It calculates the overlap by considering all the terms in both A and B.\n",
    "\n",
    "* It works even if A and B are of different sizes.\n",
    "\n",
    "* The result is always a value between 0 and 1.\n",
    "\n",
    "For two sets A and B, the Jaccard Coefficient J(A,B) is defined as:\n",
    "$J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}$\n",
    "Where:\n",
    "\n",
    "* $|A \\cap B|$ is the size of the intersection of sets A and B\n",
    "* $|A \\cup B|$ is the size of the union of sets A and B\n",
    "\n",
    "The Jaccard Coefficient always results in a value between 0 and 1, inclusive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995d3a8-1601-4d0c-95b6-57693ddf15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding the Jaccard Coefficient - Cont'd\n",
    "\n",
    "* Limitations:\n",
    "  * It doesn't account for how often a term appears in the document\n",
    "    * It doesn't recognize that rare terms can be more valuable than common ones\n",
    "    * This is why simply looking at the intersection might not always be best.\n",
    "\n",
    "* A better method is needed to adjust for length, rather than just using $|A \\cup B|$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d75aa8-d8d4-4873-93f1-2c5bcc620c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Understanding Document Similarity: Measuring Relevance\n",
    "\n",
    "* Information Retrieval (IR) aims to rank documents by relevance to a search query\n",
    "  * Finding documents similar to search criteria\n",
    "  * Ranking documents based on closeness to search terms\n",
    "  * Prioritizing most relevant results for user convenience\n",
    "\n",
    "* Approach:\n",
    "  * Assign each document a score between 0 and 1\n",
    "  * Score represents alignment (relevance) with the search query\n",
    "  * Higher score indicates greater relevance\n",
    "\n",
    "* Evolution of techniques:\n",
    "  * Document Retrieval field has developed numerous innovative solutions\n",
    "  * Continuous improvement in accuracy and efficiency of matching algorithms\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa412f63-2cc9-42a0-be40-8e80c31bd5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measuring Document Similarity: Calculating a Match Score\n",
    "\n",
    "* Basic principle of term-based scoring:\n",
    "  * Single-term search as a simple example\n",
    "  * Score of 0 if search term is absent from document\n",
    "  * Score increases with frequency of search term in document\n",
    "\n",
    "* Key factors in scoring:\n",
    "  * Presence/absence of search terms\n",
    "  * Frequency of term occurrence\n",
    "  * (Implied) More complex scoring for multi-term searches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term-Document Count Matrices\n",
    "* A count matrix displays the frequency of each word within a document.\n",
    "  * This approach is known as the \"bag of words\" model.\n",
    "* The sequence of words in the document is not taken into account.\n",
    "* For instance, the phrases \"John is quicker than Mary\" and \"Mary is quicker than John\" would produce identical vectors in this model.\n",
    "* Having precomputed count matrices makes it easy to work with large datasets, as we don't need to preprocess text each time we analyze it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf88fb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Term Frequency (tf)\n",
    "\n",
    "* Term frequency, denoted as $tf_{t,d}$, measures how often a term $t$ appears in a document $d$.\n",
    "\n",
    "* While a higher $tf$ often suggests a better match, it doesn't always directly correspond to the significance of that match:\n",
    "\n",
    "  * A document where a term appears 10 times is likely more relevant than one where it appears only once.\n",
    "  \n",
    "  * However, it's not necessarily 10 times more relevant.\n",
    "\n",
    "* This indicates that relevance doesn't scale linearly with term frequency.\n",
    "\n",
    "* The $tf$ measure helps quantify the importance of a term within a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1e540",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's a revised and cleaned-up version of the slide:\n",
    "\n",
    "### Understanding Log-Frequency Weighting\n",
    "\n",
    "* The weight of term $t$ in document $d$ is calculated using log-frequency:\n",
    "\n",
    "  $$w_{t,d} = \\begin{cases} \n",
    "  1 + \\log_{10}tf_{t,d} & \\text{if } tf_{t,d} > 0 \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}$$\n",
    "\n",
    "* This logarithmic scaling helps moderate the impact of high-frequency terms:\n",
    "  * $tf_{t,d} = 0 \\rightarrow w_{t,d} = 0$\n",
    "  * $tf_{t,d} = 1 \\rightarrow w_{t,d} = 1$\n",
    "  * $tf_{t,d} = 2 \\rightarrow w_{t,d} \\approx 1.3$\n",
    "  * $tf_{t,d} = 10 \\rightarrow w_{t,d} = 2$\n",
    "  * $tf_{t,d} = 1000 \\rightarrow w_{t,d} = 4$\n",
    "\n",
    "* To score a document-query pair, sum the weights of terms present in both:\n",
    "\n",
    "$$\\text{score} = \\sum_{t \\in q \\cap d} (1 + \\log_{10}tf_{t,d})$$\n",
    "* A score of 1 indicates no query terms are found in the document.\n",
    "\n",
    "* While variations exist, this approach captures the core idea of log-frequency weighting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e78dad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Importance of Document Frequency\n",
    "\n",
    "* The challenge of rare terms remains:\n",
    "  * Rare terms often provide more valuable information than common ones.\n",
    "    * Think of stop words as an example.\n",
    "* Take the term 'arachnid' in a query, which is seldom found in the collection:\n",
    "  * A document that includes this term is highly probable to be pertinent to the query 'arachnid'.\n",
    "  * This term significantly aids in contrasting documents effectively.\n",
    "  * Therefore, it's beneficial to assign a higher weight to infrequent terms like 'arachnid'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0aaf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuing with Document Frequency\n",
    "\n",
    "* Common terms often offer less unique information than their rarer counterparts.\n",
    "* Think of a query term that's widely seen in the collection, like `high`, `increase`, or `true`:\n",
    "  * Solely using the $tf$ score, a document with these terms seems more relevant compared to one without.\n",
    "  * However, this doesn't guarantee its significance.\n",
    "* To evaluate how often a term appears across documents, we'll determine (or normalize using) its document frequency, denoted as `df`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410223",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inverse Document Frequency (`idf`)\n",
    "\n",
    "- $\\mbox{df}_{t,d}$ represents the frequency of term $t$ within document $d$.\n",
    "- $\\mbox{df}_t$ serves as an inverse gauge of the term $t$'s informativeness.\n",
    "  * Note: $\\mbox{df}_t \\le N$, with $N$ being the entire document count.\n",
    "\n",
    "- The inverse document frequency ($\\mbox{idf}$) of term $t$ is defined as:\n",
    "$$\n",
    "idf_t = log_{10}(N/\\mbox{df}_t)\n",
    "$$\n",
    "\n",
    "- We opt for the inverse since it's more practical than handling minuscule numbers, especially when $N$ is much larger than $\\mbox{df}_t$.\n",
    "- The logarithm (`log`) is incorporated to temper the `idf` effect, which becomes especially vital when managing vast document sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55756d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term Frequency-Inverse Document Frequency (`tf-idf`) Scheme\n",
    "\n",
    "- The `tf-idf` weight of a term is derived from multiplying its term frequency (`tf`) and inverse document frequency (`idf`).\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "w_{t,d} &= \\mbox{tf}_{t,d} \\times \\mbox{idf}_t \\\\\n",
    "&= log(1+\\mbox{tf}_{t,d}) \\times log(N/\\mbox{df}_t)\n",
    "\\end{split}\n",
    "$$\n",
    "- This weighting method is a well-accepted strategy in the realm of information retrieval.\n",
    "  * Other references: tf.idf, tf x idf\n",
    "\n",
    "- The weight:\n",
    "  * Rises as a term's occurrence within a document increases.\n",
    "  * Also grows with the term's scarcity across the entire document set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955c6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Score for a Document Given a Query\n",
    "\n",
    "\n",
    "$$\n",
    "Score(Q, T) = \\sum_{t\\in Q\\cap T} \\mbox{tf}.\\mbox{idf}_{t,d}\n",
    "$$\n",
    "\n",
    "* There are many variants\n",
    "  * How `tf` is computed (with/without logs)\n",
    "  * Whether the terms in the query are also weighted\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bfc8d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using `tf-idf` for Feature Engineering\n",
    "* Each document is represented by a real-valued vector of $\\mbox{tf-idf}$ weights $\\in R^{|V|}$\n",
    "\n",
    "![](https://www.dropbox.com/s/1bx77e488ee6wek/count_tf_idf.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd99bd6b-862c-41c3-88f5-913d1012c6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_1': ['the', 'sky', 'is', 'blue'],\n",
       " 'doc_2': ['the', 'sun', 'is', 'bright'],\n",
       " 'doc_3': ['the', 'sun', 'in', 'the', 'sky', 'is', 'bright']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "docs = {\n",
    "    \"doc_1\": \"the sky is blue\",\n",
    "    \"doc_2\": \"the sun is bright\",\n",
    "    \"doc_3\": \"the sun in the sky is bright\",\n",
    "}\n",
    "\n",
    "docs = {k: v.lower().split() for k, v in docs.items()}\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd13d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(docs)\n",
    "\n",
    "def compute_tf(term, document):\n",
    "    return document.count(term)\n",
    "assert compute_tf(\"the\",  docs['doc_1']) == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b4dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_tf(\"the\",  docs['doc_3']) == 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb07f932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term= \"the\"\n",
    "\n",
    "sum(1 for document in docs.values() if term in document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "792967db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term= \"sky\"\n",
    "sum(1 for document in docs.values() if term in document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af74a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term= \"blue\"\n",
    "sum(1 for document in docs.values() if term in document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6ea9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_idf(term, documents):\n",
    "    N = len(documents)\n",
    "    df = sum(1 for document in documents if term in document)    \n",
    "    return math.log((1 + N) / (1 + df)) + 1\n",
    "\n",
    "compute_idf(\"the\", docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "280a8b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert compute_idf(\"the\", docs.values()) == math.log10(4 / 4) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dda3555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_tf_idf(term, document, documents):\n",
    "    tf = compute_tf(term, document)\n",
    "    idf = compute_idf(term, documents)\n",
    "    return round(tf * idf, 2)\n",
    "\n",
    "compute_tf_idf(\"the\",  docs[\"doc_1\"], docs.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d83e8574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.29"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf(\"sky\",  docs[\"doc_1\"], docs.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470d8cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_tf_idf(\"blue\",  docs[\"doc_1\"], docs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce64927",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vector Representation in Information Retrieval (IR)\n",
    "\n",
    "- Concept 1: Represent both queries and documents as vectors within a defined space.\n",
    "- Concept 2: Sort documents based on how close their vectors are to the query vector in this space.\n",
    "  * Here, closeness refers to vector similarity.\n",
    "    * Using Euclidean distance might be misleading especially if vectors vary in length.\n",
    "      * Large Euclidean distances can arise between vectors of dissimilar lengths.\n",
    "- Concept 3: Order documents based on the angle they form with the query vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f294685",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the king hath happily received, macbeth,\\nthe news of thy success; and when he reads\\nthy personal venture in the rebels' fight,\\nhis wonders and his praises do contend\\nwhich should be thine or his: silenced with that,\\nin viewing o'er the rest o' the selfsame day,\\nhe finds thee in the stout norweyan ranks,\\nnothing afeard of what thyself didst make,\\nstrange images of death. as thick as hail\\ncame post with post; and every one did bear\\nthy praises in his kingdom's great defence,\\nand pour'd them down before him.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_1 = \"\"\"The king hath happily received, Macbeth,\n",
    "The news of thy success; and when he reads\n",
    "Thy personal venture in the rebels' fight,\n",
    "His wonders and his praises do contend\n",
    "Which should be thine or his: silenced with that,\n",
    "In viewing o'er the rest o' the selfsame day,\n",
    "He finds thee in the stout Norweyan ranks,\n",
    "Nothing afeard of what thyself didst make,\n",
    "Strange images of death. As thick as hail\n",
    "Came post with post; and every one did bear\n",
    "Thy praises in his kingdom's great defence,\n",
    "And pour'd them down before him.\"\"\"\n",
    "\n",
    "doc_1 = doc_1.lower()\n",
    "doc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4b8ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the king was really happy to hear about your success, macbeth. \\nwhen he read about your brave actions fighting the rebels, he was so impressed he couldn't\\ndecide who to praise more, you or himself. that same day, he noticed you also stood bravely\\nagainst the norwegians and not scared at all, even when facing dangerous situations. many\\nmessengers came, one after the other, all of them talking about how great you were in defending\\nthe kingdom. they all praised you in front of the king.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_2 =  \"\"\"The king was really happy to hear about your success, Macbeth. \n",
    "When he read about your brave actions fighting the rebels, he was so impressed he couldn't\n",
    "decide who to praise more, you or himself. That same day, he noticed you also stood bravely\n",
    "against the Norwegians and not scared at all, even when facing dangerous situations. Many\n",
    "messengers came, one after the other, all of them talking about how great you were in defending\n",
    "the kingdom. They all praised you in front of the king.\"\"\"\n",
    "\n",
    "doc_2 = doc_2.lower()\n",
    "doc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2503ca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "lorem_ipsum = \"\"\"\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \n",
    "eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut \n",
    "enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat. Ut \n",
    "enim ad minim veniam, quis nostrud exercitation ullamco laboris \n",
    "nisi ut aliquip ex ea commodo consequat. \n",
    "\"\"\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec146570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [doc_1, doc_2,  doc_1 + lorem_ipsum]\n",
    "\n",
    "compute_tf_idf(\"KING\",  corpus[0], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bae9fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"the king hath happily received, macbeth,\\nthe news of thy success; and when he reads\\nthy personal venture in the rebels' fight,\\nhis wonders and his praises do contend\\nwhich should be thine or his: silenced with that,\\nin viewing o'er the rest o' the selfsame day,\\nhe finds thee in the stout norweyan ranks,\\nnothing afeard of what thyself didst make,\\nstrange images of death. as thick as hail\\ncame post with post; and every one did bear\\nthy praises in his kingdom's great defence,\\nand pour'd them down before him.\",\n",
       " \"the king was really happy to hear about your success, macbeth. \\nwhen he read about your brave actions fighting the rebels, he was so impressed he couldn't\\ndecide who to praise more, you or himself. that same day, he noticed you also stood bravely\\nagainst the norwegians and not scared at all, even when facing dangerous situations. many\\nmessengers came, one after the other, all of them talking about how great you were in defending\\nthe kingdom. they all praised you in front of the king.\",\n",
       " \"the king hath happily received, macbeth,\\nthe news of thy success; and when he reads\\nthy personal venture in the rebels' fight,\\nhis wonders and his praises do contend\\nwhich should be thine or his: silenced with that,\\nin viewing o'er the rest o' the selfsame day,\\nhe finds thee in the stout norweyan ranks,\\nnothing afeard of what thyself didst make,\\nstrange images of death. as thick as hail\\ncame post with post; and every one did bear\\nthy praises in his kingdom's great defence,\\nand pour'd them down before him.\\nlorem ipsum dolor sit amet, consectetur adipiscing elit, sed do \\neiusmod tempor incididunt ut labore et dolore magna aliqua. ut \\nenim ad minim veniam, quis nostrud exercitation ullamco laboris \\nnisi ut aliquip ex ea commodo consequat. ut \\nenim ad minim veniam, quis nostrud exercitation ullamco laboris \\nnisi ut aliquip ex ea commodo consequat. \\n\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dc70c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0, 1.29, 4.0, 5.15, 0.0, 0.0]\n",
      "[4.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "[2.0, 1.29, 4.0, 5.15, 1.69, 3.39]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = ['king', 'happily', 'and', \"thy\", \"ipsum\", \"laboris\"]\n",
    "print([compute_tf_idf(v,  corpus[0], corpus) for v in vocabulary])\n",
    "print([compute_tf_idf(v,  corpus[1], corpus) for v in vocabulary])\n",
    "print([compute_tf_idf(v,  corpus[2], corpus) for v in vocabulary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae86391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63fe0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "693ea1a1-4360-44f1-a997-20681a21d666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(vocabulary=[&#x27;king&#x27;, &#x27;happily&#x27;, &#x27;and&#x27;, &#x27;thy&#x27;, &#x27;ipsum&#x27;,\n",
       "                            &#x27;laboris&#x27;])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(vocabulary=[&#x27;king&#x27;, &#x27;happily&#x27;, &#x27;and&#x27;, &#x27;thy&#x27;, &#x27;ipsum&#x27;,\n",
       "                            &#x27;laboris&#x27;])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(vocabulary=['king', 'happily', 'and', 'thy', 'ipsum',\n",
       "                            'laboris'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "c_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68b53b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 4, 3, 0, 0],\n",
       "        [2, 0, 1, 0, 0, 0],\n",
       "        [1, 1, 4, 3, 1, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vec.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f53600c0-60ca-45e1-82e6-6907d14f6a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see king 1 times \n",
      "I see happily 1 times \n",
      "I see and 4 times \n",
      "I see thy 3 times \n",
      "I see ipsum 0 times \n",
      "I see laboris 0 times \n"
     ]
    }
   ],
   "source": [
    "[]\n",
    "for v in vocabulary:\n",
    "    cnt = corpus[0].split().count(v)\n",
    "    print(f\"I see {v} {cnt} times \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df7c588c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 4, 3, 0, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[corpus[0].lower().split().count(v) for v in vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb1eaf57-edaf-42e6-95cf-9c1837dcf8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vec = TfidfVectorizer(vocabulary=vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c3f546b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be1c10f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17, 0.22, 0.69, 0.67, 0.  , 0.  ],\n",
       "       [0.89, 0.  , 0.45, 0.  , 0.  , 0.  ],\n",
       "       [0.14, 0.19, 0.58, 0.56, 0.24, 0.49]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit_transform(corpus).todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "34751cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18, 0.21, 0.73, 0.62, 0.0, 0.0]\n",
      "[0.71, 0.0, 0.71, 0.0, 0.0, 0.0]\n",
      "[0.16, 0.18, 0.65, 0.55, 0.21, 0.42]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_tf(term, document):\n",
    "    return document.lower().split().count(term) / len(document.lower().split() )\n",
    "\n",
    "def compute_idf(term, documents):\n",
    "    N = len(documents)\n",
    "    df = sum(1 for document in documents if term in document)\n",
    "    return math.log10((1+N) / (1 + df)) + 1\n",
    "\n",
    "\n",
    "def compute_tf_idf(document, terms, idfs):\n",
    "    tf_idf_raw = [compute_tf(term, document) * idfs.get(term, 0) for term in terms]\n",
    "    norm = np.linalg.norm(tf_idf_raw, 2)  # L2 norm\n",
    "    tf_idf_normalized = [round(value / norm, 2) for value in tf_idf_raw]\n",
    "    \n",
    "    return tf_idf_normalized\n",
    "\n",
    "\n",
    "idfs = {term: compute_idf(term, corpus) for term in vocabulary}\n",
    "\n",
    "for doc in corpus:\n",
    "    tf_idf_values = compute_tf_idf(doc, vocabulary, idfs)\n",
    "    print(tf_idf_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c2311402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'document', 'first', 'is', 'second', 'the']"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92cfeb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 4, 3, 0, 0],\n",
       "       [2, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "c_vec = CountVectorizer(vocabulary=vocabulary)\n",
    "X = c_vec.fit_transform(corpus).toarray()\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14f68c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "23348560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11111111, 0.11111111, 0.44444444, 0.33333333, 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]/X[0].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49cd958c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.11111111, 0.44444444, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.66666667, 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08333333, 0.08333333, 0.33333333, 0.25      , 0.08333333,\n",
       "        0.16666667]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X / np.sum(X, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d590b57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.11111111, 0.44444444, 0.33333333, 0.        ,\n",
       "        0.        ],\n",
       "       [0.66666667, 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08333333, 0.08333333, 0.33333333, 0.25      , 0.08333333,\n",
       "        0.16666667]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = X / np.sum(X, axis=1, keepdims=True)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a033afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 4, 3, 0, 0],\n",
       "       [2, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 4, 3, 1, 2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d081cc5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 3, 2, 1, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = np.sum(X > 0, axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6af43add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = X.shape[0]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3f385b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.28768207, 1.        , 1.28768207, 1.69314718,\n",
       "       1.69314718])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = np.log((1 + N) / (1 + df)) + 1\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "411dbd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11111111, 0.14307579, 0.44444444, 0.42922736, 0.        ,\n",
       "        0.        ],\n",
       "       [0.66666667, 0.        , 0.33333333, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.08333333, 0.10730684, 0.33333333, 0.32192052, 0.1410956 ,\n",
       "        0.2821912 ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = tf * idf\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c50ca875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e47330a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04b517d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26726124, 0.53452248, 0.80178373])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x / np.sqrt(sum(x**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "897a0ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26726124, 0.53452248, 0.80178373]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([x], norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4e36fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000008622003"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0.26726124, 0.53452248, 0.80178373])\n",
    "np.sqrt(sum(y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d6ab08df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26726124, 0.53452248, 0.80178373]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize([z], norm='l2', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "68356d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.17 0.22 0.69 0.67 0.   0.  ]\n",
      " [0.89 0.   0.45 0.   0.   0.  ]\n",
      " [0.14 0.19 0.58 0.56 0.24 0.49]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = normalize(tfidf, norm='l2', axis=1)\n",
    "print(tfidf.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ef71d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Angles to Cosines\n",
    "\n",
    "* In information retrieval, the following two notions are equivalent.\n",
    "  * Rank documents in decreasing order of the angle between query and hit\n",
    "  * Rank documents in increasing order of cosine(query,hit)\n",
    "\n",
    "* Cosine is a monotonically decreasing function for the interval [0o, 180o]\n",
    "\n",
    "![](https://www.dropbox.com/s/lpq4vvnlnmz0oxw/cosine.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f75b10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Length Normalization\n",
    "\n",
    "* A vector can be (length-) normalized by dividing each of its components by its length \n",
    "  * We commonly use the $L2$ norm:\n",
    "\n",
    "* Dividing a vector by its $L2$ norm makes it a unit (length) vector\n",
    "\n",
    "  * Effect on the two documents $d$ and $d′$ (d appended to itself) have identical vectors after length-normalization.\n",
    "  * Thus, long and short documents now have comparable weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d7d3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similairity\n",
    "\n",
    "* $q_i$ is the `tf-idf` weight of term `i` in the query\n",
    "* $d_i$ is the `tf-idf` weight of term `i` in the document\n",
    "\n",
    "![](https://www.dropbox.com/s/4x1fb50xiqidmnf/cos_equation.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalized TF-IDF Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9036e0ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cosine Similarity Illustrated\n",
    "\n",
    "![](https://www.dropbox.com/s/4inqt6nf9mfz6h9/cosine_similarity.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0fac1",
   "metadata": {},
   "source": [
    "### Example \n",
    "* Books: \"Sense and Sensibility\", \"Pride and Prejudice\", \"Wuthering Heights?\".\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/z28xu8xxhuv8ll5/example_books.png?dl=1\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "```\n",
    "cos(SaS,PaP) ≈ 0.789 × 0.832 + 0.515 × 0.555 + 0.335 × 0.0 + 0.0 × 0.0 ≈ 0.94\n",
    "cos(SaS,WH) ≈ 0.79\n",
    "cos(PaP,WH) ≈ 0.69\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a65d3d",
   "metadata": {},
   "source": [
    "### `tf-idf` Weighing  Variants\n",
    "\n",
    "* Just an FYI\n",
    "<div align=\"center\">\n",
    "<img src=\"https://www.dropbox.com/s/r88cmbmaqyk7hcp/weighting_schemes.png?dl=1\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "</div>\n",
    "* identifies components of the SMART notation: combination in use in a search engine (ddd.qqq)\n",
    "  * e.g.: lnc.ltc\n",
    "  * ```To the legacy of the SMART system belongs the so-called SMART triple notation, a mnemonic scheme for denoting tf-idf weighting variants in the vector space model. The mnemonic for representing a combination of weights takes the form ddd.qqq, where the first three letters represents the term weighting of the collection document vector and the second three letters represents the term weighting for the query document vector. For example, ltc.lnn represents the ltc weighting applied to a collection document and the lnn weighting applied to a query document.```  https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System\n",
    "  \n",
    "* See [Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/) for more info if you're interested in the topic\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b261d-517b-4ace-91fd-fb3d18489efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
