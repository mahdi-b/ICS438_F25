{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Word2Vec: A Better Word Embedding\n",
    "\n",
    "- **Issues with tf-idf Embeddings**:\n",
    "  - **Long and Sparse Vectors**: These vectors are often too long and filled with near-zero values.\n",
    "  - Not Ideal for Machine Learning: Due to their length and sparsity, they lead to more model parameters and longer training times.\n",
    "  - Word Order & Synonyms: Tf-idf doesn't consider the order of words or handle synonyms well.\n",
    "\n",
    "- How Do We Address These Issues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Using Taxonomy to Capture Similarity\n",
    "\n",
    "- Overcoming the lack of context can be achieved using taxonomies.\n",
    "- What's a Taxonomy? \n",
    "  - It's a \"knowledge organization system\" that is often visualized as a tree or graph. \n",
    "  - It categorizes terms within a field and the relationships between them.\n",
    "  - Taxonomies vary: from broad (like English Language taxonomy) to specific (like Computing or Amazon Product Taxonomy).\n",
    "  \n",
    "- A popular example is the WordNet Taxonomy:\n",
    "  - Contains 155,327 English words.\n",
    "  - [More about WordNet on Wikipedia](https://en.wikipedia.org/wiki/WordNet)\n",
    "\n",
    "![WordNet Taxonomy Example](https://www.dropbox.com/s/9kapn0eq6v84g2m/word_net_taxonomy.png?dl=1)\n",
    "[Source](https://escholarship.org/content/qt9j8221x8/qt9j8221x8.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Semantic Similarity\n",
    "\n",
    "* WordNet organizes words into sets of synonyms, known as synsets. \n",
    "  * Each synset represents a distinct concept.\n",
    "\n",
    "* Hypernymy and Hyponymy Relations:\n",
    "  * WordNet provides hypernyms (more general terms) and hyponyms (more specific terms) for each word.\n",
    "  * Dog is a `hypernym` of Canine\n",
    "  * Poodle is a `hyponym` of Dog\n",
    "\n",
    "* For each word in your dataset, create a feature vector where each entry corresponds to the semantic similarity between the word and a set of predefined words (or reference words). \n",
    "  * This will reduce the sparsity, as each word is now represented as a vector of similarity scores.\n",
    "\n",
    "* Meronyms and Holonyms:\n",
    "    * Consider incorporating part-whole relationships (meronyms) and whole-part relationships (holonyms) as additional features. For example, 'wheel' is a meronym of 'car'.\n",
    "      * Branch is a `meronym` of tree\n",
    "      * Forest is a `holonym` of tree    \n",
    "* Ideally we want to be able to **learn** a word's representation, such that the representation encodes meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Beyond Taxonomies: The Case for Word2Vec\n",
    "\n",
    "- Constructing taxonomies is intricate and demand expert input.\n",
    "- Taxonomies offer clear hierarchies, but a word's context in text often shapes its meaning.\n",
    "  - Unlike dynamic embeddings, taxonomies can restrict a word to a single definition.\n",
    "    - Bank can mean many different things (verb, geographical entity, financial institution, etc.) \n",
    "- Representing Distances: Knowing \"snake\" is a \"reptile\" is useful, but how close is it to \"lizard\"? Taxonomies might fall short.\n",
    "  \n",
    "* While taxonomies provide organized structure, their rigidity limits comprehensive word representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Understanding Word2Vec: Turning Word Meanings into Math\n",
    "\n",
    "- When we talk about a word's 'meaning', we're referring to the concept or idea it stands for. \n",
    "  \n",
    "- Now, imagine if we could translate these concepts into numbers to use in formulas.\n",
    "  * This means we can actually do calculations with word meanings!\n",
    "  \n",
    "- Here's an ineteresting example: \n",
    "  If - we do the math with the concepts behind the words, such as: \n",
    "  King minus Man plus Woman, the answer should remind us of the word 'Queen'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/hiqe4ql1ne9reu2/mwkq.png?dl=1\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Drawing Parallels: Word Operations vs. Color Operations\n",
    "\n",
    "- We already perform operations on words that represent colors, much like math.\n",
    "\n",
    "- Think about how we mix and match colors. Can't we do the same with words?\n",
    "  - For instance:\n",
    "    - Mixing red and green gives us yellow.\n",
    "    - Subtracting magenta from blue results in cyan.\n",
    "    - Yellow brings to mind bananas more than it does green.\n",
    "    - Combine black and white, and you get a shade of grey.\n",
    "    - Just as 'royal' is a shade of yellow, 'sky' is a shade of blue.\n",
    "\n",
    "[![Color Operations](https://www.dropbox.com/s/aon76xh7qlu1z2y/colors.png?dl=1)](https://www.dropbox.com/s/aon76xh7qlu1z2y/colors.png?dl=1)\n",
    "\n",
    "Reference: [Exploring word color associations](https://gist.github.com/aparrish/2f562e3737544\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Drawing Parallels: Word Operations vs. Color Operations\n",
    "\n",
    "- We already perform operations on words that represent colors, much like math.\n",
    "\n",
    "- Think about how we mix and match colors. Can't we do the same with words?\n",
    "  - For instance:\n",
    "    - Mixing red and green gives us yellow.\n",
    "    - Subtracting magenta from blue results in cyan.\n",
    "    - Yellow brings to mind bananas more than it does green.\n",
    "    - Combine black and white, and you get a shade of grey.\n",
    "    - Just as 'royal' is a shade of yellow, 'sky' is a shade of blue.\n",
    "\n",
    "[![Color Operations](https://www.dropbox.com/s/aon76xh7qlu1z2y/colors.png?dl=1)](https://www.dropbox.com/s/aon76xh7qlu1z2y/colors.png?dl=1)\n",
    "\n",
    "Reference: [Exploring word color associations](https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "![](https://www.dropbox.com/s/5eh77i3byy0av50/xkcd%20colors.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "import json\n",
    "color_data = json.loads(open(\"media/xkcd.json\").read())\n",
    "print(color_data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_data[\"colors\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '#acc2d9'\n",
    "from textwrap import wrap\n",
    "wrap(x[1:], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {col_info[\"color\"]:tuple(wrap(col_info[\"hex\"][1:], 2)) for col_info in color_data[\"colors\"]}\n",
    "colors[\"cloudy blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "int('ac', 16), int('c2', 16), int('d9', 16), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {name:np.array(list(int(hex_v, 16) for hex_v in hex_t)) for name,hex_t in colors.items()}\n",
    "colors[\"cloudy blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These colors were manually labelled by participants\")\n",
    "\n",
    "print(f\"Black is {colors['black']}, white is: {colors['white']} and red is {colors['red']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "![](https://www.dropbox.com/s/9k2828pyr0nypla/red.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3]) + np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3]) - np.array([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# Compute the Euclidean Distance in numpy\n",
    "def dist(coord1, coord2):\n",
    "    # Euclidean distance in numpy. \n",
    "    return np.linalg.norm(coord1 - coord2)\n",
    "    \n",
    "dist(colors['red'], colors['blue'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([[1,2,3], [1,2,3], [1,2,3]], axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(colors['red'], colors['green']) > dist(colors['cherry red'], colors['tomato red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(query, colors, n=10):\n",
    "    closest = []\n",
    "    closest = sorted(colors.keys(),\n",
    "                        key=lambda x: dist(query, colors[x]))[:n]\n",
    "    return closest\n",
    "\n",
    "closest(colors['red'], colors,  n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Manipulating Colors with Their RGB \"Embeddings\"\n",
    "\n",
    "- Using these RGB \"embeddings\", we can perform arithmetic on colors, much like we do with numbers.\n",
    "  - Think of these operations:\n",
    "     * Adding red and green gives us yellow. \n",
    "     * Subtracting blue from magenta leaves us with red.\n",
    "     * When we talk about proximity, yellow is nearer to royal than to green in the color spectrum.\n",
    "     * Mix black and white in equal parts, and you'll land on grey.\n",
    "     * In the same way \"banana\" relates to the color yellow, \"hunter green\" relates to the basic shade of green.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Red + green = yellow  \n",
    "some_color = colors[\"red\"] + colors[\"green\"]\n",
    "closest(some_color, colors, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_color = colors[\"magenta\"] - colors[\"blue\"]\n",
    "closest(some_color, colors, n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist(colors[\"yellow\"], colors[\"banana\"]) < dist(colors[\"yellow\"], colors[\"green\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_color =  np.mean([colors['black'], colors['white']], axis=0)\n",
    "closest(some_color, colors,  n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Relationship Between Colors\n",
    "\n",
    "* Banana yellow is to yellow what hunter green is to green\n",
    "  * Derived from the exact diagram\n",
    "\n",
    "![](https://www.dropbox.com/s/aon76xh7qlu1z2y/colors.png?dl=1)\n",
    "\n",
    "```\n",
    "some_color = colors['yellow'] - colors['banana'] + colors['green']\n",
    "closest(colors, some_color, n=5)\n",
    "\n",
    "['true green',\n",
    " 'grassy green',\n",
    " 'vibrant green',\n",
    " 'grass green',\n",
    " 'dark grass green']\n",
    "```\n",
    "![](https://www.dropbox.com/s/tjgnw6cwf0kwju8/green_prediction.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_color = colors['yellow'] - colors['banana'] + colors['green']\n",
    "\n",
    "closest(some_color, colors,  n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Making the Jump: Colors to Words\n",
    "\n",
    "- Remember how we did math with colors? That's possible because each color has a meaningful numeric representation, or an \"embedding\".\n",
    "- Similarly, words can have their own numeric representations and when words with similar meanings (semantically similar) get close numeric values, we say they have good \"embeddings\".\n",
    "\n",
    "- Enter Word2Vec: it gives each word a unique numeric vector. When you do math on these vectors, the results tell us about the relationships between words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Understanding Word Embeddings\n",
    "\n",
    "- Word embeddings are machine-interpretable numerical representations, analogous to how colors can be encoded as RGB values\n",
    "\n",
    "- Words with similar semantic meanings have embeddings that are mathematically close in vector space, while words with different meanings have embeddings that are further apart\n",
    "\n",
    "- This embedding concept extends beyond individual words - we can create vector representations for sentences, documents, and even non-linguistic data like protein sequences and images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Understanding Word2Vec\n",
    "\n",
    "- \"You shall know a word by the company it keeps\" - J.R. Firth\n",
    "  - Words derive their semantic meaning from their contextual relationships\n",
    "\n",
    "- Consider: \"Paris is a city and the ___ of France.\"\n",
    "  - Among options like 'pretzel', 'pizza', 'capital', or 'painting', \n",
    "  - 'capital' fits naturally because it appears frequently in similar contexts\n",
    "\n",
    "- Word2Vec learns semantic relationships by predicting words that co-occur in similar contexts\n",
    "  - This distributional hypothesis is fundamental to computational linguistics\n",
    "\n",
    "- By capturing these contextual patterns, Word2Vec generates vector representations that preserve semantic relationships, enabling machines to process natural language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Word2Vec and Language Models: Key Distinctions\n",
    "\n",
    "- Language models serve as foundational components in natural language processing systems, enabling various comprehension tasks\n",
    "\n",
    "- Language models predict the probability distribution of words or tokens in a sequence:\n",
    "  - Given \"How are you ...\", they compute probabilities for subsequent words like \"doing\",  \"feeling\" or \"pizza\"\n",
    "  - They model both local and long-range dependencies in text\n",
    "\n",
    "```\n",
    "Language modeling is the task of assigning a probability to sentences in a language. [...] Besides assigning a probability to each sequence of words, the language models also assign a probability for the likelihood of a given word (or a sequence of words) to follow a sequence of words.\n",
    "```\n",
    "- Source: Neural Network Methods in Natural Language Processing\n",
    "\n",
    "- While Word2Vec shares some application similarities with language models, it differs in its primary objective:\n",
    "  - Language models: Predict probability distributions over entire vocabularies\n",
    "  - Word2Vec: Generate dense vector representations that capture semantic relationships between words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Word2Vec: Core Mechanism\n",
    "\n",
    "- Word2Vec operates on large text corpora  and the algorithm assigns each vocabulary word an n-dimensional vector in continuous vector space\n",
    "\n",
    "- For each word occurrence in the corpus:\n",
    "  - A center word is selected\n",
    "  - A context window defines neighboring words within a specified distance\n",
    "  - The relationship between center and context words informs the learning process\n",
    "\n",
    "- The training objective:\n",
    "  - Given context words, predict the center word (Skipgram model)\n",
    "  - Or predict context words given the center word (CBOW model)\n",
    "  - Vectors are iteratively optimized through gradient descent to maximize prediction accuracy\n",
    "\n",
    "- Through this iterative optimization process, the resulting word vectors capture meaningful semantic relationships in the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Word2Vec Process\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/i9686ozir426221/process.png?dl=1\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Word2Vec Visualized\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/sfkstxxyeevpiau/data_2d_space.png?dl=1\" width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### How Capitals Relate to Countries: An Example\n",
    "\n",
    "![Capital Relationships](https://www.dropbox.com/s/mwdh6z9qc0pflyy/capitals_example.png?dl=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### The  Skipgram Algorithm\n",
    "![](https://www.dropbox.com/s/ykyjsroxu1utwd0/skipgram.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### The Continuous Bag of Words ( CBOW) Algorithm\n",
    "![](https://www.dropbox.com/s/sae7f1sp84xuwwy/cbow.png?dl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Word2Vec and Modern Embedding Architectures: A Comparative Analysis\n",
    "\n",
    "- Word2Vec generates efficient, static word embeddings\n",
    "  - Requires only unlabeled text corpora for training\n",
    "  - Computationally efficient for basic NLP tasks\n",
    "\n",
    "- Contemporary architectures produce contextual embeddings\n",
    "  - Leverage transformer-based architectures\n",
    "  - Generate dynamic representations based on surrounding context\n",
    "\n",
    "- The embedding paradigm extends beyond word-level analysis\n",
    "  - Encompasses sentence, document, and multi-modal representations\n",
    "  - Notable examples include Facebook AI's Speech2Vec for audio embeddings\n",
    "\n",
    "- Recent advances in deep learning have significantly enhanced embedding quality\n",
    "  - Improved semantic and syntactic relationship capture\n",
    "  - Greater precision in downstream NLP tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
