{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fasttext\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### NPR Media Dialog Dataset Overview\n",
    "\n",
    "* Dataset Specifications (npr.org archives):\n",
    "  * 140,000+ NPR radio interview transcripts\n",
    "  * 20-year temporal coverage\n",
    "  * 10,000+ hours of transcribed audio content\n",
    "\n",
    "\n",
    "\n",
    "* Available via Kaggle platform\n",
    "  * [kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts](kaggle.com/datasets/shuyangli94/interview-npr-media-dialog-transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./media/npr_1000_utterances.csv\", 'r') as f:\n",
    "    i = 0 \n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        if i ==3:\n",
    "            break\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "reader = csv.reader(open(\"./media/npr_1000_utterances.csv\"), delimiter=',', quotechar='\"')\n",
    "for row in reader:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We will use `simple_preprocess` to   lowercases, tokenizes, de-accent the a string.\n",
    "The output of `simple_preprocess` are final tokens = unicode strings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"WWW.google.com So #$test in ~every! time! What?\"\n",
    "\n",
    "gensim.utils.simple_preprocess(some_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    utterances = []\n",
    "    with open(input_file, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            text = row[-1]  \n",
    "            yield gensim.utils.simple_preprocess(text)  # Yield the preprocessed text\n",
    "\n",
    "utterances = list(read_input(\"./media/npr_1000_utterances.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(utterances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances[20][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We will use `gensim` to train a `Word2Vec` model on the 1000 utterances\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(utterances, window=10, min_count=2, workers=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.get(\"washington\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.get(\"chicago\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.get(\"tokyo\") == None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Question 1.\n",
    "In the code above, we see that Washington is set to index 406, and Chicago is set to index 124. Why is Tokyo set to None?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"washington\"].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv[\"washington\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"washington\"]\n",
    "model.wv.most_similar(positive=w1, topn=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Question 2. \n",
    "When searching for the words most similar to 'Washington', we get results like 'his', 'from', etc., which are clearly not semantically similar to the word 'Washington'. Why is that? Didn't we show that Word2Vec does a good job of grouping semantically similar words, like city names?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Some issues with the embeddings\n",
    "\n",
    "-- Add text after pracitcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = list(read_input(\"./media/npr_100000_utterances.csv\"))\n",
    "model = gensim.models.Word2Vec(utterances, window=10, min_count=2, workers=10)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"peace\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = [\"clean\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3.\n",
    "\n",
    "Why do the embeddings seem more specific now? Can you explain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Facebook's FastText\n",
    "```FastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "![](https://www.dropbox.com/s/i74guibnv5mxx2h/fasttext.png?dl=1)\n",
    "\n",
    "https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv wiki-news-300d-1M.vec.zip media/\n",
    "!unzip -f media/wiki-news-300d-1M.vec.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ./media/words_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_words= [x.rstrip() for x in open(\"./media/words_to_keep\")]\n",
    "keep_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words_embeds = {}\n",
    "for line in open(\"media/wiki-news-300d-1M.vec\"):\n",
    "    data = line.split()\n",
    "    if data[0] in keep_words:\n",
    "        words_embeds[data[0]] = np.array(list(map(float, data[1:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeds[\"big\"].size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = words_embeds[\"girl\"] - words_embeds[\"boy\"] + words_embeds[\"brother\"]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res - words_embeds[\"sister\"]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeds[\"big\"] - words_embeds[\"bigger\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeds[\"bad\"] - words_embeds[\"worse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "(words_embeds[\"bad\"] - words_embeds[\"worse\"]) - (words_embeds[\"big\"] - words_embeds[\"bigger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/faiss\n",
    "# !pip install faiss\n",
    "dbutils.fs.ls(\"dbfs:/FileStore/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(list(words_embeds.keys()))\n",
    "embeds = np.array(list(words_embeds.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(300)\n",
    "index.add(embeds)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([words_embeds[\"bad\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.search(np.array([words_embeds[\"bad\"]]), k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Demo from Faiss: The Missing Manual\n",
    "- Demo posted on Pinecone's website\n",
    "\n",
    "  - [Faiss: The Missing Manual](https://www.pinecone.io/learn/series/faiss/faiss-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
